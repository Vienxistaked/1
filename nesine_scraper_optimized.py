"""
Nesine.com Futbol MaÃ§larÄ± - BÃ¼lten & Puan Tablosu Scraper
Hibrit mimari: Selenium (sayfa yÃ¼kleme) + BeautifulSoup (veri Ã§ekme)
"""

import csv
import logging
import os
import time
from dataclasses import dataclass, asdict
from typing import Optional, List, Dict
from bs4 import BeautifulSoup, Tag
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException


# Logging yapÄ±landÄ±rmasÄ±
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    datefmt="%H:%M:%S"
)
logger = logging.getLogger(__name__)


@dataclass
class TeamStanding:
    """TakÄ±m puan tablosu verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    MaÃ§: Optional[str] = None
    TakÄ±m_Tipi: Optional[str] = None
    SÄ±ra: Optional[str] = None
    TakÄ±m: Optional[str] = None
    O: Optional[str] = None
    G: Optional[str] = None
    B: Optional[str] = None
    M: Optional[str] = None
    A_Y: Optional[str] = None
    AV: Optional[str] = None
    P: Optional[str] = None
    Form: Optional[str] = None
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


@dataclass
class LastMatch:
    """Son maÃ§ verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    GÃ¼ncel_MaÃ§: Optional[str] = None
    TakÄ±m: Optional[str] = None
    TakÄ±m_Tipi: Optional[str] = None
    Lig: Optional[str] = None
    Tarih: Optional[str] = None
    Ev_Sahibi: Optional[str] = None
    Deplasman: Optional[str] = None
    MS: Optional[str] = None
    Ä°Y: Optional[str] = None
    SonuÃ§: Optional[str] = None  # Galibiyet, MaÄŸlubiyet, Beraberlik
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


@dataclass
class RefereeMatch:
    """Hakem maÃ§ verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    GÃ¼ncel_MaÃ§: Optional[str] = None
    Hakem_AdÄ±: Optional[str] = None
    Lig: Optional[str] = None
    Tarih: Optional[str] = None
    Ev_Sahibi: Optional[str] = None
    Deplasman: Optional[str] = None
    MS: Optional[str] = None
    Ä°Y: Optional[str] = None
    Oran_1: Optional[str] = None
    Oran_1_Geldi: Optional[str] = None
    Oran_X: Optional[str] = None
    Oran_X_Geldi: Optional[str] = None
    Oran_2: Optional[str] = None
    Oran_2_Geldi: Optional[str] = None
    Oran_Alt: Optional[str] = None
    Oran_Alt_Geldi: Optional[str] = None
    Oran_Ãœst: Optional[str] = None
    Oran_Ãœst_Geldi: Optional[str] = None
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


@dataclass
class RefereeStats:
    """Hakem istatistik verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    GÃ¼ncel_MaÃ§: Optional[str] = None
    Hakem_AdÄ±: Optional[str] = None
    MS1_SayÄ±: Optional[str] = None
    MS1_YÃ¼zde: Optional[str] = None
    MSX_SayÄ±: Optional[str] = None
    MSX_YÃ¼zde: Optional[str] = None
    MS2_SayÄ±: Optional[str] = None
    MS2_YÃ¼zde: Optional[str] = None
    Alt_2_5_SayÄ±: Optional[str] = None
    Alt_2_5_YÃ¼zde: Optional[str] = None
    Ãœst_2_5_SayÄ±: Optional[str] = None
    Ãœst_2_5_YÃ¼zde: Optional[str] = None
    KG_Var_SayÄ±: Optional[str] = None
    KG_Var_YÃ¼zde: Optional[str] = None
    KG_Yok_SayÄ±: Optional[str] = None
    KG_Yok_YÃ¼zde: Optional[str] = None
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


@dataclass
class CompetitionHistory:
    """Rekabet geÃ§miÅŸi verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    GÃ¼ncel_MaÃ§: Optional[str] = None
    Lig: Optional[str] = None
    Tarih: Optional[str] = None
    Ev_Sahibi: Optional[str] = None
    Deplasman: Optional[str] = None
    MS: Optional[str] = None
    Ä°Y: Optional[str] = None
    Oran_1: Optional[str] = None
    Oran_1_Geldi: Optional[str] = None
    Oran_X: Optional[str] = None
    Oran_X_Geldi: Optional[str] = None
    Oran_2: Optional[str] = None
    Oran_2_Geldi: Optional[str] = None
    Oran_Alt: Optional[str] = None
    Oran_Alt_Geldi: Optional[str] = None
    Oran_Ãœst: Optional[str] = None
    Oran_Ãœst_Geldi: Optional[str] = None
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


@dataclass
class InjuryData:
    """Sakat ve cezalÄ± oyuncu verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    MaÃ§: Optional[str] = None
    TakÄ±m: Optional[str] = None
    Numara: Optional[str] = None
    Oyuncu: Optional[str] = None
    YaÅŸ: Optional[str] = None
    Pozisyon: Optional[str] = None
    MaÃ§_SayÄ±sÄ±: Optional[str] = None
    Ä°lk_11: Optional[str] = None
    Gol: Optional[str] = None
    Asist: Optional[str] = None
    Durum: Optional[str] = None  # "SakatlÄ±k" veya "CezalÄ±"
    AÃ§Ä±klama: Optional[str] = None  # DetaylÄ± aÃ§Ä±klama
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


@dataclass
class MatchData:
    """MaÃ§ verilerini tutan veri sÄ±nÄ±fÄ±."""
    MaÃ§_Kodu: Optional[str] = None
    Lig: Optional[str] = None
    Tarih: Optional[str] = None
    Saat: Optional[str] = None
    MaÃ§: Optional[str] = None
    MBS: Optional[str] = None
    MS_1: Optional[str] = None
    MS_X: Optional[str] = None
    MS_2: Optional[str] = None
    Alt_2_5: Optional[str] = None
    Ãœst_2_5: Optional[str] = None
    HND: Optional[str] = None
    HND_1: Optional[str] = None
    HND_X: Optional[str] = None
    HND_2: Optional[str] = None
    Ã‡S_1X: Optional[str] = None
    Ã‡S_12: Optional[str] = None
    Ã‡S_X2: Optional[str] = None
    KG_Var: Optional[str] = None
    KG_Yok: Optional[str] = None
    Market_SayÄ±sÄ±: Optional[str] = None
    Ä°statistik_Link: Optional[str] = None
    
    def to_dict(self) -> Dict[str, str]:
        return {k: (v if v is not None else "") for k, v in asdict(self).items()}


class NesineScraper:
    """Nesine.com bÃ¼lten ve puan tablosu verilerini Ã§eken scraper."""
    
    # le=0 â†’ tÃ¼m ligler dahil (le=2 sadece editÃ¶r seÃ§kisi, daha az maÃ§)
    BASE_URL: str = "https://www.nesine.com/iddaa"
    DEFAULT_PARAMS: Dict[str, str] = {"et": "1", "le": "0"}
    TIMEOUT: int = 15
    
    def __init__(self, match_count: int) -> None:
        self.match_count: int = match_count
        self.driver: Optional[webdriver.Chrome] = None
        self.matches: List[MatchData] = []
        self._seen_codes: set = set()  # Duplicate koruma (incremental collection)
        self._last_league_info: Dict[str, str] = {"league": None, "date": None}
        self.standings: List[TeamStanding] = []
        self.competition_history: List[CompetitionHistory] = []
        self.last_matches: List[LastMatch] = []
        self.referee_matches: List[RefereeMatch] = []
        self.referee_stats: List[RefereeStats] = []
        self.injury_data: List[InjuryData] = []
        
    def setup_driver(self) -> None:
        """Chrome WebDriver'Ä± headless modda yapÄ±landÄ±rÄ±r."""
        options = webdriver.ChromeOptions()
        options.add_argument("--headless=new")
        options.add_argument("--window-size=1920,1080")
        options.add_argument("--disable-notifications")
        options.add_argument("--disable-popup-blocking")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        
        self.driver = webdriver.Chrome(options=options)
        logger.info("WebDriver baÅŸlatÄ±ldÄ± (headless)")
        
    def wait_for_page_load(self) -> None:
        """SayfanÄ±n yÃ¼klenmesini bekler."""
        wait = WebDriverWait(self.driver, self.TIMEOUT)
        wait.until(EC.presence_of_element_located(
            (By.CSS_SELECTOR, "div[data-test-id^='r_'][data-code]")
        ))
        logger.info("BÃ¼lten verileri yÃ¼klendi")
        
    def close_popups(self) -> None:
        """Popup'larÄ± JavaScript ile kapatÄ±r."""
        popup_scripts = [
            """
            const cookieBtn = document.evaluate(
                "//button[contains(text(), 'Kabul Et')]",
                document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null
            ).singleNodeValue;
            if (cookieBtn) cookieBtn.click();
            """,
            """
            const closeBtn = document.querySelector('button[class*="ebfa54f068cb6c89755a"]');
            if (closeBtn) closeBtn.click();
            """,
            """
            const kapatBtn = document.evaluate(
                "//button[contains(text(), 'Kapat')]",
                document, null, XPathResult.FIRST_ORDERED_NODE_TYPE, null
            ).singleNodeValue;
            if (kapatBtn) kapatBtn.click();
            """,
            """
            document.querySelectorAll('button i.ni-close-rounded').forEach(i => {
                i.closest('button')?.click();
            });
            """
        ]
        
        for script in popup_scripts:
            try:
                self.driver.execute_script(script)
            except WebDriverException:
                pass
                
        logger.info("Popup'lar kapatÄ±ldÄ±")
        
    def scroll_to_load_matches(self) -> None:
        """Legacy wrapper â€” artÄ±k _scroll_and_collect kullanÄ±lÄ±yor."""
        self._scroll_and_collect()

    # â”€â”€ Scroll Container AlgÄ±lama (DOM Virtualization Fix) â”€â”€â”€â”€â”€â”€â”€â”€â”€
    _SCROLL_CONTAINER_JS: str = """
    // Nesine.com 'overflow: hidden' body kullanÄ±r.
    // GerÃ§ek scroll container'Ä± bulmak iÃ§in tÃ¼m ancestor'larÄ± tarayÄ±p
    // scrollHeight > clientHeight olan ve overflow auto/scroll set edilmiÅŸ
    // ilk elementi dÃ¶ndÃ¼rÃ¼yoruz.
    (function findScrollContainer() {
        // Strateji 1: MaÃ§ satÄ±rlarÄ±nÄ±n en yakÄ±n scrollable parent'Ä±
        const firstRow = document.querySelector("div[data-test-id^='r_'][data-code]");
        if (firstRow) {
            let el = firstRow.parentElement;
            while (el && el !== document.documentElement) {
                const style = window.getComputedStyle(el);
                const oy = style.overflowY;
                if ((oy === 'auto' || oy === 'scroll') && el.scrollHeight > el.clientHeight + 50) {
                    return el;
                }
                el = el.parentElement;
            }
        }
        // Strateji 2: class adÄ±nda 'scroll' geÃ§en div'ler
        const candidates = document.querySelectorAll("div[class*='scroll'], div[class*='Scroll']");
        for (const c of candidates) {
            if (c.scrollHeight > c.clientHeight + 50) return c;
        }
        // Strateji 3: scrollHeight > clientHeight olan en bÃ¼yÃ¼k div
        let best = null; let bestDelta = 0;
        document.querySelectorAll('div').forEach(d => {
            const delta = d.scrollHeight - d.clientHeight;
            if (delta > 200 && delta > bestDelta) {
                const s = window.getComputedStyle(d);
                if (s.overflowY !== 'visible' && s.overflowY !== 'hidden') {
                    best = d; bestDelta = delta;
                }
            }
        });
        if (best) return best;
        // Strateji 4 (fallback): body
        return document.body;
    })();
    """

    def _find_scroll_container(self) -> None:
        """Sayfadaki gerÃ§ek scroll container'Ä± tespit edip JS referansÄ±nÄ± kaydeder.

        ``window.__nsnScrollContainer`` global deÄŸiÅŸkenine atanÄ±r.
        Sonraki scroll iÅŸlemlerinde bu referans kullanÄ±lÄ±r.
        """
        self.driver.execute_script(
            f"window.__nsnScrollContainer = {self._SCROLL_CONTAINER_JS}"
        )
        # Hangi elementin bulunduÄŸunu logla
        tag_info: str = self.driver.execute_script("""
            const c = window.__nsnScrollContainer;
            return c.tagName + '.' + (c.className || '').substring(0, 60)
                   + ' [scrollH=' + c.scrollHeight + ', clientH=' + c.clientHeight + ']';
        """)
        logger.info(f"  Scroll container: {tag_info}")

    def _wait_for_new_rows(self, old_count: int, timeout: float = 8.0) -> int:
        """DOM'a yeni maÃ§ satÄ±rÄ± eklenene veya XHR bitene kadar bekler.

        ``time.sleep`` yerine Explicit Wait kullanÄ±r.
        ``WebDriverWait`` + custom expected_condition ile
        ``div[data-test-id^='r_'][data-code]`` sayÄ±sÄ±nÄ±n artmasÄ±nÄ± bekler.

        Parameters
        ----------
        old_count : int
            Scroll Ã¶ncesi DOM'daki satÄ±r sayÄ±sÄ±.
        timeout : float
            Maksimum bekleme sÃ¼resi (saniye).

        Returns
        -------
        int
            Bekleme sonrasÄ± DOM'daki gÃ¼ncel satÄ±r sayÄ±sÄ±.
        """
        try:
            WebDriverWait(self.driver, timeout).until(
                lambda d: len(d.find_elements(
                    By.CSS_SELECTOR, "div[data-test-id^='r_'][data-code]"
                )) > old_count
            )
        except TimeoutException:
            pass  # Timeout â†’ mevcut sayÄ±yla devam et

        # Network idle bekleme: bekleyen XHR/Fetch sayÄ±sÄ± 0 olana kadar
        try:
            self.driver.execute_async_script("""
                const cb = arguments[arguments.length - 1];
                // 500ms boyunca yeni network isteÄŸi gelmezse tamam say
                let timer = null;
                const done = () => { clearTimeout(timer); cb(true); };
                timer = setTimeout(done, 500);
            """)
        except (TimeoutException, WebDriverException):
            pass

        return len(self.driver.find_elements(
            By.CSS_SELECTOR, "div[data-test-id^='r_'][data-code]"
        ))

    def _scroll_and_collect(self) -> None:
        """Scroll â†’ Explicit Wait â†’ Parse â†’ Retry dÃ¶ngÃ¼sÃ¼ ile maÃ§ toplar.

        DOM Virtualization-Resistant Strateji:
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. _find_scroll_container() ile gerÃ§ek scrollable elementi bul
        2. Her adÄ±mda son maÃ§ satÄ±rÄ±na scrollIntoView() yap (container
           tabanlÄ± scroll + gÃ¶rÃ¼nÃ¼r elemana odaklanma)
        3. _wait_for_new_rows() ile Explicit Wait (yeni DOM node bekleme)
        4. Her adÄ±mda page_source â†’ parse â†’ _seen_codes ile incremental
        5. Stale olursa bÃ¼yÃ¼k sÄ±Ã§rama + retry mekanizmasÄ±
        6. match_count'a ulaÅŸÄ±lmadÄ±kÃ§a ASLA erken Ã§Ä±kma

        Retry MekanizmasÄ±:
          â€¢ Stale (yeni maÃ§ gelmeme) sayacÄ±
          â€¢ MAX_RETRY_AFTER_STALE: Sayfa sonundan emin olunca bile
            retry denemesi (tam sayfa yeniden yÃ¼kleme dahil)
          â€¢ scrollIntoView fallback: container scroll baÅŸarÄ±sÄ±z olursa
        """
        SCROLL_STEP: int = 1200           # px â€” dÃ¼ÅŸÃ¼k tutarak virtualization kaÃ§Ä±rmasÄ±nÄ± azalt
        MAX_STALE_ROUNDS: int = 6         # ArdÄ±ÅŸÄ±k yeni veri gelmeyen turlar
        MAX_TOTAL_SCROLLS: int = 500      # GÃ¼venlik sÄ±nÄ±rÄ±
        MAX_RETRY_AFTER_STALE: int = 3    # Sayfa sonu sonrasÄ± toplam retry hakkÄ±
        AGGRESSIVE_JUMP: int = 4000       # Stale durumda bÃ¼yÃ¼k sÄ±Ã§rama (px)

        stale_rounds: int = 0
        total_scrolls: int = 0
        prev_collected: int = 0
        retry_count: int = 0

        logger.info("BÃ¼lten verileri Ã§ekiliyor (smart scroll + explicit wait)...")
        logger.info("-" * 60)

        # â”€â”€ Scroll container'Ä± tespit et â”€â”€
        self._find_scroll_container()

        # â”€â”€ Ä°lk yÃ¼kleme: mevcut DOM'daki tÃ¼m maÃ§larÄ± topla â”€â”€
        time.sleep(1.5)
        soup = self.get_page_source()
        self.get_match_data(soup)
        logger.info(f"  Ä°lk yÃ¼kleme: {len(self.matches)} maÃ§ toplandÄ±")

        while total_scrolls < MAX_TOTAL_SCROLLS:
            # â”€â”€ Hedef kontrolÃ¼ â”€â”€
            if len(self.matches) >= self.match_count:
                logger.info(
                    f"âœ“ Hedef ulaÅŸÄ±ldÄ±: {len(self.matches)}/{self.match_count} maÃ§"
                )
                break

            # â”€â”€ Mevcut DOM satÄ±r sayÄ±sÄ± (Explicit Wait referansÄ±) â”€â”€
            current_dom_count: int = len(self.driver.find_elements(
                By.CSS_SELECTOR, "div[data-test-id^='r_'][data-code]"
            ))

            # â”€â”€ SCROLL: Ã‡ift strateji (container + scrollIntoView) â”€â”€
            try:
                # Strateji A: GerÃ§ek scroll container'Ä± kaydÄ±r
                self.driver.execute_script(
                    f"window.__nsnScrollContainer.scrollTop += {SCROLL_STEP};"
                )
            except WebDriverException:
                pass

            try:
                # Strateji B: Son gÃ¶rÃ¼nÃ¼r maÃ§ satÄ±rÄ±na scrollIntoView
                # (DOM Virtualization altÄ±nda en gÃ¼venilir yÃ¶ntem)
                self.driver.execute_script("""
                    const rows = document.querySelectorAll(
                        "div[data-test-id^='r_'][data-code]"
                    );
                    if (rows.length > 0) {
                        rows[rows.length - 1].scrollIntoView({
                            behavior: 'instant', block: 'end'
                        });
                    }
                """)
            except WebDriverException:
                pass

            total_scrolls += 1

            # â”€â”€ Explicit Wait: Yeni satÄ±rlar yÃ¼klenene kadar bekle â”€â”€
            new_dom_count: int = self._wait_for_new_rows(
                current_dom_count, timeout=8.0
            )

            # â”€â”€ Parse et & topla â”€â”€
            soup = self.get_page_source()
            self.get_match_data(soup)

            if len(self.matches) > prev_collected:
                stale_rounds = 0
                logger.info(
                    f"  ğŸ“Š Scroll #{total_scrolls}: "
                    f"toplanan={len(self.matches)}/{self.match_count} "
                    f"(DOM={new_dom_count})"
                )
                prev_collected = len(self.matches)
            else:
                stale_rounds += 1

            # â”€â”€ Sayfa sonu kontrolÃ¼ (scroll container tabanlÄ±) â”€â”€
            at_bottom: bool = self.driver.execute_script("""
                const c = window.__nsnScrollContainer;
                return (c.scrollTop + c.clientHeight) >= (c.scrollHeight - 150);
            """)

            if stale_rounds >= MAX_STALE_ROUNDS:
                if at_bottom and retry_count < MAX_RETRY_AFTER_STALE:
                    # â”€â”€ RETRY: Sayfa sonu ama hedef sayÄ±ya ulaÅŸÄ±lmadÄ± â”€â”€
                    retry_count += 1
                    logger.warning(
                        f"  âš  Sayfa sonuna ulaÅŸÄ±ldÄ± ama hedef uzak "
                        f"({len(self.matches)}/{self.match_count}). "
                        f"Retry {retry_count}/{MAX_RETRY_AFTER_STALE}..."
                    )
                    # SayfayÄ± baÅŸtan yÃ¼klemeyip container'Ä± en Ã¼ste sarmak
                    # ve tekrar aÅŸaÄŸÄ± scroll etmek virtualized DOM'u
                    # yeniden render ettirebilir.
                    self.driver.execute_script("""
                        const c = window.__nsnScrollContainer;
                        c.scrollTop = 0;
                    """)
                    time.sleep(2)
                    # Tekrar en alta kaydÄ±r (bu sefer adÄ±m adÄ±m)
                    self.driver.execute_script("""
                        const c = window.__nsnScrollContainer;
                        c.scrollTop = c.scrollHeight;
                    """)
                    time.sleep(3)
                    soup = self.get_page_source()
                    self.get_match_data(soup)

                    if len(self.matches) > prev_collected:
                        prev_collected = len(self.matches)
                        stale_rounds = 0
                        logger.info(
                            f"  âœ“ Retry baÅŸarÄ±lÄ±: {len(self.matches)} maÃ§"
                        )
                        continue

                elif at_bottom and retry_count >= MAX_RETRY_AFTER_STALE:
                    # TÃ¼m retry haklarÄ± tÃ¼kendi â€” gerÃ§ekten sayfa sonu
                    logger.warning(
                        f"Sayfa fiziksel olarak sona erdi â€” toplanan: "
                        f"{len(self.matches)}/{self.match_count}"
                    )
                    break
                else:
                    # Sayfa sonunda deÄŸiliz â†’ agresif sÄ±Ã§rama dene
                    logger.debug(
                        f"  Stale #{stale_rounds}: Agresif sÄ±Ã§rama deneniyor"
                    )
                    try:
                        self.driver.execute_script(
                            f"window.__nsnScrollContainer.scrollTop += {AGGRESSIVE_JUMP};"
                        )
                    except WebDriverException:
                        pass
                    time.sleep(2)
                    # Ek fallback: tÃ¼m container'Ä± tazelemek iÃ§in sayfa
                    # boyutunu deÄŸiÅŸtirip geri al (DOM re-render tetikler)
                    try:
                        self.driver.execute_script("""
                            const rows = document.querySelectorAll(
                                "div[data-test-id^='r_'][data-code]"
                            );
                            if (rows.length > 0) {
                                rows[rows.length - 1].scrollIntoView({
                                    behavior: 'instant', block: 'center'
                                });
                            }
                        """)
                    except WebDriverException:
                        pass
                    time.sleep(1)
                    soup = self.get_page_source()
                    self.get_match_data(soup)
                    stale_rounds = stale_rounds // 2  # KÄ±smen sÄ±fÄ±rla

        logger.info(
            f"Scroll tamamlandÄ±: {total_scrolls} adÄ±m, "
            f"{len(self.matches)} maÃ§ toplandÄ±"
        )

    def get_page_source(self) -> BeautifulSoup:
        """Sayfa kaynaÄŸÄ±nÄ± BeautifulSoup ile parse eder."""
        return BeautifulSoup(self.driver.page_source, "lxml")
        
    @staticmethod
    def extract_odd(row: Tag, testid: str) -> Optional[str]:
        """Oran deÄŸerini data-testid ile Ã§eker."""
        btn = row.select_one(f'button[data-testid="{testid}"]')
        if btn:
            odd_divs = btn.select("div > div")
            for div in odd_divs:
                text = div.get_text(strip=True)
                if text and text.replace(".", "").replace(",", "").isdigit():
                    return text
                if ":" in text:
                    return text
        return None
        
    def parse_match_row(self, row: Tag, league_info: Dict[str, str]) -> Optional[MatchData]:
        """Tek bir maÃ§ satÄ±rÄ±nÄ± parse eder."""
        try:
            match_code = row.get("data-code")
            if not match_code:
                return None
                
            teams_elem = row.select_one('a[data-test-id="matchName"]')
            teams = teams_elem.get_text(strip=True) if teams_elem else None
            stats_link = teams_elem.get("href") if teams_elem else None
            
            time_elem = row.select_one('span[data-testid^="time-"]')
            match_time = time_elem.get_text(strip=True) if time_elem else None
            
            mbs_elem = row.select_one('div[data-test-id="event_mbs"] span')
            mbs = mbs_elem.get_text(strip=True) if mbs_elem else None
            
            market_elem = row.select_one(f'div[data-test-id="{match_code}_m"]')
            market_count = market_elem.get_text(strip=True) if market_elem else None
            
            return MatchData(
                MaÃ§_Kodu=match_code,
                Lig=league_info.get("league"),
                Tarih=league_info.get("date"),
                Saat=match_time,
                MaÃ§=teams,
                MBS=mbs,
                MS_1=self.extract_odd(row, "odd_MaÃ§ Sonucu_1"),
                MS_X=self.extract_odd(row, "odd_MaÃ§ Sonucu_X"),
                MS_2=self.extract_odd(row, "odd_MaÃ§ Sonucu_2"),
                Alt_2_5=self.extract_odd(row, "odd_2,5 Gol_Alt"),
                Ãœst_2_5=self.extract_odd(row, "odd_2,5 Gol_Ãœst"),
                HND=self.extract_odd(row, "odd_HandikaplÄ± MaÃ§ Sonucu_HND"),
                HND_1=self.extract_odd(row, "odd_HandikaplÄ± MaÃ§ Sonucu_1"),
                HND_X=self.extract_odd(row, "odd_HandikaplÄ± MaÃ§ Sonucu_X"),
                HND_2=self.extract_odd(row, "odd_HandikaplÄ± MaÃ§ Sonucu_2"),
                Ã‡S_1X=self.extract_odd(row, "odd_Ã‡ifte Åans_1-X"),
                Ã‡S_12=self.extract_odd(row, "odd_Ã‡ifte Åans_1-2"),
                Ã‡S_X2=self.extract_odd(row, "odd_Ã‡ifte Åans_X-2"),
                KG_Var=self.extract_odd(row, "odd_KarÅŸ. Gol_Var"),
                KG_Yok=self.extract_odd(row, "odd_KarÅŸ. Gol_Yok"),
                Market_SayÄ±sÄ±=market_count,
                Ä°statistik_Link=stats_link
            )
            
        except Exception as e:
            logger.error(f"MaÃ§ parse hatasÄ±: {e}")
            return None
            
    def extract_league_info(self, container: Tag) -> Dict[str, str]:
        """Lig ve tarih bilgisini container'dan Ã§Ä±karÄ±r."""
        league = None
        date = None
        
        league_elem = container.select_one("strong")
        if league_elem:
            league = league_elem.get_text(strip=True)
            
        date_elem = container.select_one('div[data-test-id="date"]')
        if date_elem:
            date = date_elem.get_text(strip=True)
            
        return {"league": league, "date": date}
        
    def get_match_data(self, soup: BeautifulSoup) -> None:
        """BeautifulSoup ile tÃ¼m maÃ§ verilerini Ã§eker.

        v2.1 Robust Parsing:
          â€¢ Birincil yol: div[data-item-index] container tabanlÄ± traverse
            (lig/tarih header â†’ maÃ§ satÄ±rlarÄ± iliÅŸkisi korunur)
          â€¢ Yedek yol: Container bulunamazsa veya yeterli maÃ§ yoksa,
            doÄŸrudan tÃ¼m div[data-test-id^='r_'][data-code] satÄ±rlarÄ±nÄ±
            tara ve her satÄ±r iÃ§in en yakÄ±n lig header'Ä± bul
          â€¢ Duplicate koruma: instance-level _seen_codes set ile mÃ¼kerrer
            engelleme (incremental collection Ã§aÄŸrÄ±larÄ± arasÄ±nda korunur)
        """

        # â”€â”€ Birincil Yol: Container tabanlÄ± traverse â”€â”€
        containers = soup.select('div[data-item-index]')

        for container in containers:
            if len(self.matches) >= self.match_count:
                break

            new_league_info = self.extract_league_info(container)
            if new_league_info["league"]:
                self._last_league_info["league"] = new_league_info["league"]
            if new_league_info["date"]:
                self._last_league_info["date"] = new_league_info["date"]

            match_rows = container.select('div[data-test-id^="r_"][data-code]')

            for row in match_rows:
                if len(self.matches) >= self.match_count:
                    break

                code = row.get("data-code")
                if code in self._seen_codes:
                    continue

                match_data = self.parse_match_row(row, self._last_league_info)

                if match_data and match_data.MaÃ§:
                    self._seen_codes.add(code)
                    self.matches.append(match_data)
                    logger.info(
                        f"âœ“ [{len(self.matches)}/{self.match_count}] "
                        f"{match_data.MaÃ§}"
                    )

        # â”€â”€ Yedek Yol: DÃ¼z satÄ±r taramasÄ± (container eksikse) â”€â”€
        if len(self.matches) < self.match_count:
            all_rows = soup.select('div[data-test-id^="r_"][data-code]')
            new_in_fallback = 0

            for row in all_rows:
                if len(self.matches) >= self.match_count:
                    break

                code = row.get("data-code")
                if code in self._seen_codes:
                    continue

                # En yakÄ±n lig header'Ä±nÄ± bul (Ã¶nceki sibling'lerde)
                league_info = self._find_nearest_league_header(row)

                match_data = self.parse_match_row(row, league_info)

                if match_data and match_data.MaÃ§:
                    self._seen_codes.add(code)
                    self.matches.append(match_data)
                    new_in_fallback += 1
                    logger.info(
                        f"âœ“ [{len(self.matches)}/{self.match_count}] "
                        f"{match_data.MaÃ§}"
                    )

            if new_in_fallback > 0:
                logger.info(
                    f"  Fallback tarama: {new_in_fallback} ek maÃ§ bulundu"
                )

    def _find_nearest_league_header(self, row: Tag) -> Dict[str, str]:
        """Bir maÃ§ satÄ±rÄ±nÄ±n en yakÄ±n lig/tarih header'Ä±nÄ± bulur.

        DOM'da yukarÄ± doÄŸru traverse ederek league header arar.
        Virtualized DOM'da container kaybolmuÅŸ olabilir; bu durumda
        parent ve Ã¶nceki sibling'lerden bilgi Ã§Ä±karmaya Ã§alÄ±ÅŸÄ±r.
        """
        info: Dict[str, str] = {"league": None, "date": None}

        # Parent container'Ä± dene
        parent = row.parent
        for _ in range(5):  # Max 5 seviye yukarÄ± Ã§Ä±k
            if parent is None:
                break
            league_elem = parent.select_one("strong")
            date_elem = parent.select_one('div[data-test-id="date"]')
            if league_elem:
                info["league"] = league_elem.get_text(strip=True)
            if date_elem:
                info["date"] = date_elem.get_text(strip=True)
            if info["league"]:
                break
            parent = parent.parent

        # HÃ¢lÃ¢ bulunamadÄ±ysa Ã¶nceki sibling'lerden dene
        if not info["league"]:
            prev = row.find_previous("strong")
            if prev:
                info["league"] = prev.get_text(strip=True)
        if not info["date"]:
            prev_date = row.find_previous(attrs={"data-test-id": "date"})
            if prev_date:
                info["date"] = prev_date.get_text(strip=True)

        return info
                    
    def parse_standing_row(self, row: Tag, match_code: str, match_name: str, team_type: str) -> Optional[TeamStanding]:
        """Puan tablosu satÄ±rÄ±nÄ± parse eder."""
        try:
            # SÄ±ra numarasÄ±
            rank_elem = row.select_one('td[data-test-id="renderSortNumberColumn"] span:last-child')
            rank = rank_elem.get_text(strip=True) if rank_elem else None
            
            # TakÄ±m adÄ±
            team_elem = row.select_one('a[data-test-id="TeamLink"]')
            team_name = team_elem.get_text(strip=True) if team_elem else None
            
            # Ä°statistikler
            o_elem = row.select_one('td.oCol[data-test-id="renderDefaultColumn"]')
            g_elem = row.select_one('td.gCol[data-test-id="renderDefaultColumn"]')
            b_elem = row.select_one('td.bCol[data-test-id="renderDefaultColumn"]')
            m_elem = row.select_one('td.mCol[data-test-id="renderDefaultColumn"]')
            ay_elem = row.select_one('td.ayCol[data-test-id="renderDefaultColumn"]')
            av_elem = row.select_one('td.avCol[data-test-id="renderDefaultColumn"]')
            p_elem = row.select_one('td.pCol[data-test-id="renderDefaultColumn"]')
            
            # Form (son maÃ§lar)
            form_elems = row.select('span[data-test-id="getResultTooltipValue"]')
            form_list = [f.get_text(strip=True) for f in form_elems if f.get_text(strip=True) != "?"]
            form = "".join(form_list) if form_list else None
            
            return TeamStanding(
                MaÃ§_Kodu=match_code,
                MaÃ§=match_name,
                TakÄ±m_Tipi=team_type,
                SÄ±ra=rank,
                TakÄ±m=team_name,
                O=o_elem.get_text(strip=True) if o_elem else None,
                G=g_elem.get_text(strip=True) if g_elem else None,
                B=b_elem.get_text(strip=True) if b_elem else None,
                M=m_elem.get_text(strip=True) if m_elem else None,
                A_Y=ay_elem.get_text(strip=True) if ay_elem else None,
                AV=av_elem.get_text(strip=True) if av_elem else None,
                P=p_elem.get_text(strip=True) if p_elem else None,
                Form=form
            )
            
        except Exception as e:
            logger.error(f"Puan tablosu satÄ±r parse hatasÄ±: {e}")
            return None
            
    def find_team_standing(self, soup: BeautifulSoup, team_name: str, match_code: str, match_full_name: str, team_type: str) -> Optional[TeamStanding]:
        """Belirli bir takÄ±mÄ±n puan tablosu verisini bulur."""
        rows = soup.select('tr[data-test-id="PointTable"]')
        
        # Arama iÃ§in takÄ±m adÄ±nÄ± normalize et
        search_name = team_name.lower().strip()
        search_words = search_name.replace(".", " ").replace("-", " ").split()
        
        best_match = None
        best_score = 0
        
        for row in rows:
            team_elem = row.select_one('a[data-test-id="TeamLink"]')
            if not team_elem:
                continue
                
            table_team_name = team_elem.get_text(strip=True).lower()
            table_words = table_team_name.replace(".", " ").replace("-", " ").split()
            
            score = 0
            
            # Tam eÅŸleÅŸme
            if search_name == table_team_name:
                return self.parse_standing_row(row, match_code, match_full_name, team_type)
                
            # Ä°Ã§erme kontrolÃ¼
            if search_name in table_team_name or table_team_name in search_name:
                score = 10
                
            # Kelime eÅŸleÅŸmeleri
            for sw in search_words:
                if len(sw) >= 3:  # KÄ±sa kelimeleri atla
                    for tw in table_words:
                        if sw == tw:
                            score += 5
                        elif sw in tw or tw in sw:
                            score += 3
                            
            # Ä°lk kelime bonus
            if search_words and table_words:
                if search_words[0] == table_words[0]:
                    score += 8
                elif len(search_words[0]) >= 3 and search_words[0] in table_words[0]:
                    score += 4
                    
            if score > best_score:
                best_score = score
                best_match = row
                
        # En iyi eÅŸleÅŸme varsa dÃ¶ndÃ¼r (minimum skor 3)
        if best_match and best_score >= 3:
            return self.parse_standing_row(best_match, match_code, match_full_name, team_type)
            
        return None
        
    def match_team_name(self, search_name: str, table_name: str) -> bool:
        """Ä°ki takÄ±m adÄ±nÄ±n eÅŸleÅŸip eÅŸleÅŸmediÄŸini kontrol eder."""
        search_lower = search_name.lower().strip()
        table_lower = table_name.lower().strip()
        
        # Tam eÅŸleÅŸme
        if search_lower == table_lower:
            return True
            
        # Ä°Ã§erme kontrolÃ¼
        if search_lower in table_lower or table_lower in search_lower:
            return True
            
        # Kelime bazlÄ± eÅŸleÅŸme
        search_words = search_lower.replace(".", " ").replace("-", " ").split()
        table_words = table_lower.replace(".", " ").replace("-", " ").split()
        
        # En az bir anlamlÄ± kelime eÅŸleÅŸmesi
        for sw in search_words:
            if len(sw) >= 3:
                for tw in table_words:
                    if len(tw) >= 3:
                        if sw == tw:
                            return True
                        # KÄ±saltma kontrolÃ¼ (Ã¶rn: "Sarsfield" ve "Velez Sarsfield")
                        if len(sw) >= 4 and (sw in tw or tw in sw):
                            return True
                            
        return False

    def get_standings_for_match(self, match: MatchData) -> None:
        """Bir maÃ§ iÃ§in her iki takÄ±mÄ±n puan tablosu verilerini Ã§eker."""
        if not match.Ä°statistik_Link or not match.MaÃ§:
            return
            
        try:
            # Puan tablosu URL'si
            stats_url = f"{match.Ä°statistik_Link}/puan-tablosu"
            self.driver.get(stats_url)
            
            # Puan tablosunun yÃ¼klenmesini bekle
            wait = WebDriverWait(self.driver, self.TIMEOUT)
            wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, 'table[data-test-id="PointTableWrapper"]')
            ))
            
            soup = BeautifulSoup(self.driver.page_source, "lxml")
            
            # TakÄ±m isimlerini ayÄ±r
            teams = match.MaÃ§.split(" - ")
            if len(teams) != 2:
                logger.warning(f"TakÄ±m isimleri ayrÄ±ÅŸtÄ±rÄ±lamadÄ±: {match.MaÃ§}")
                return
                
            home_team = teams[0].strip()
            away_team = teams[1].strip()
            
            # Highlighted satÄ±rlarÄ± bul (maÃ§taki takÄ±mlar iÅŸaretli)
            highlighted_rows = soup.select('tr[data-test-id="PointTable"][class*="fe8a09b89be114afe977"], tr[data-test-id="PointTable"][class*="ba36c2fc08832e02ac89"]')
            
            home_standing = None
            away_standing = None
            home_row = None
            away_row = None
            
            # Ã–nce kesin eÅŸleÅŸmeleri bul
            for row in highlighted_rows:
                team_elem = row.select_one('a[data-test-id="TeamLink"]')
                if not team_elem:
                    continue
                    
                row_team_name = team_elem.get_text(strip=True)
                
                # Ev sahibi eÅŸleÅŸmesi kontrolÃ¼
                if not home_row and self.match_team_name(home_team, row_team_name):
                    home_row = row
                    continue
                    
                # Deplasman eÅŸleÅŸmesi kontrolÃ¼  
                if not away_row and self.match_team_name(away_team, row_team_name):
                    away_row = row
                    
            # EÄŸer 2 highlight var ve sadece 1'i eÅŸleÅŸtiyse, diÄŸeri otomatik olarak diÄŸer takÄ±m
            if len(highlighted_rows) == 2:
                if home_row and not away_row:
                    # DiÄŸer highlight away takÄ±mÄ±
                    away_row = highlighted_rows[0] if highlighted_rows[1] == home_row else highlighted_rows[1]
                elif away_row and not home_row:
                    # DiÄŸer highlight home takÄ±mÄ±
                    home_row = highlighted_rows[0] if highlighted_rows[1] == away_row else highlighted_rows[1]
                elif not home_row and not away_row:
                    # HiÃ§biri eÅŸleÅŸmediyse, sÄ±rayla ata (fuzzy ile devam et)
                    pass
                    
            # Row'lardan standing oluÅŸtur
            if home_row:
                home_standing = self.parse_standing_row(home_row, match.MaÃ§_Kodu, match.MaÃ§, "Ev Sahibi")
            if away_row:
                away_standing = self.parse_standing_row(away_row, match.MaÃ§_Kodu, match.MaÃ§, "Deplasman")
                    
            # Highlight ile bulunamadÄ±ysa tÃ¼m tabloda fuzzy ara
            if not home_standing:
                home_standing = self.find_team_standing(soup, home_team, match.MaÃ§_Kodu, match.MaÃ§, "Ev Sahibi")
            if not away_standing:
                away_standing = self.find_team_standing(soup, away_team, match.MaÃ§_Kodu, match.MaÃ§, "Deplasman")
                
            # SonuÃ§larÄ± kaydet
            if home_standing:
                self.standings.append(home_standing)
                logger.info(f"  â”œâ”€ Ev Sahibi: {home_standing.TakÄ±m} (SÄ±ra: {home_standing.SÄ±ra}, P: {home_standing.P})")
            else:
                logger.warning(f"  â”œâ”€ Ev sahibi bulunamadÄ±: {home_team}")
                
            if away_standing:
                self.standings.append(away_standing)
                logger.info(f"  â””â”€ Deplasman: {away_standing.TakÄ±m} (SÄ±ra: {away_standing.SÄ±ra}, P: {away_standing.P})")
            else:
                logger.warning(f"  â””â”€ Deplasman bulunamadÄ±: {away_team}")
                
        except TimeoutException:
            logger.error(f"Puan tablosu yÃ¼klenemedi: {match.MaÃ§}")
        except Exception as e:
            logger.error(f"Puan tablosu hatasÄ± ({match.MaÃ§}): {e}")
            
    def save_matches_to_csv(self, filename: str = "BÃ¼lten.csv") -> str:
        """BÃ¼lten verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.matches:
            logger.warning("Kaydedilecek bÃ¼lten verisi yok!")
            return filepath
            
        fieldnames = list(MatchData.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
            writer.writeheader()
            writer.writerows([m.to_dict() for m in self.matches])
            
        logger.info(f"âœ“ {len(self.matches)} maÃ§ verisi kaydedildi: {filename}")
        return filepath
        
    def save_standings_to_csv(self, filename: str = "Puan_Tablosu.csv") -> str:
        """Puan tablosu verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.standings:
            logger.warning("Kaydedilecek puan tablosu verisi yok!")
            return filepath
            
        fieldnames = list(TeamStanding.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
            writer.writeheader()
            writer.writerows([s.to_dict() for s in self.standings])
            
        logger.info(f"âœ“ {len(self.standings)} takÄ±m puan verisi kaydedildi: {filename}")
        return filepath
        
    def get_competition_history_for_match(self, match: MatchData) -> None:
        """Bir maÃ§ iÃ§in rekabet geÃ§miÅŸi verilerini Ã§eker."""
        if not match.Ä°statistik_Link or not match.MaÃ§:
            return
            
        try:
            # Rekabet geÃ§miÅŸi URL'si
            stats_url = f"{match.Ä°statistik_Link}/rekabet-gecmisi"
            self.driver.get(stats_url)
            
            # Rekabet geÃ§miÅŸi tablosunun yÃ¼klenmesini bekle
            wait = WebDriverWait(self.driver, self.TIMEOUT)
            wait.until(EC.presence_of_element_located(
                (By.CSS_SELECTOR, 'div[data-test-id="CompitionHistoryTable"]')
            ))
            
            soup = BeautifulSoup(self.driver.page_source, "lxml")
            
            # Rekabet geÃ§miÅŸi satÄ±rlarÄ±nÄ± bul
            history_rows = soup.select('div[data-test-id="CompitionHistoryTableItem"]')
            
            for row in history_rows:
                history_data = self.parse_competition_history_row(row, match.MaÃ§_Kodu, match.MaÃ§)
                if history_data:
                    self.competition_history.append(history_data)
                    
            logger.info(f"  â””â”€ {len(history_rows)} geÃ§miÅŸ maÃ§ bulundu")
                
        except TimeoutException:
            logger.warning(f"  â””â”€ Rekabet geÃ§miÅŸi bulunamadÄ±")
        except Exception as e:
            logger.error(f"Rekabet geÃ§miÅŸi hatasÄ± ({match.MaÃ§}): {e}")
            
    def parse_competition_history_row(self, row: Tag, match_code: str, current_match: str) -> Optional[CompetitionHistory]:
        """Rekabet geÃ§miÅŸi satÄ±rÄ±nÄ± parse eder."""
        try:
            # Lig bilgisi
            league_elem = row.select_one('span[data-test-id="CompitionTableItemLeague"] span:first-child')
            league = league_elem.get_text(strip=True) if league_elem else None
            
            # Tarih bilgisi
            date_elem = row.select_one('span[data-test-id="CompitionTableItemSeason"]')
            date = date_elem.get_text(strip=True) if date_elem else None
            
            # Ev sahibi takÄ±m
            home_elem = row.select_one('div[data-test-id="HomeTeam"] a span')
            home_team = home_elem.get_text(strip=True) if home_elem else None
            
            # Deplasman takÄ±m
            away_elem = row.select_one('div[data-test-id="AwayTeam"] a span')
            away_team = away_elem.get_text(strip=True) if away_elem else None
            
            # MaÃ§ sonucu
            score_elem = row.select_one('button[data-test-id="NsnButton"] span')
            score = score_elem.get_text(strip=True) if score_elem else None
            
            # Ä°lk yarÄ± sonucu
            first_half_elem = row.select_one('span[data-test-id="CompitionTableItemFirstHalf"]')
            first_half = first_half_elem.get_text(strip=True) if first_half_elem else None
            
            # Oranlar
            odds_container = row.select_one('div[data-test-id="CompitionTableItemOdds"]')
            odds = odds_container.select('span[data-test-id="CompitionHistoryTableItem"]') if odds_container else []
            
            # Kazanan oran class'Ä±: ab18fc768d1ec03e3ada
            winning_class = "ab18fc768d1ec03e3ada"
            
            # OranlarÄ± parse et
            odd_1 = odds[0].get_text(strip=True) if len(odds) > 0 else None
            odd_1_won = "Evet" if (len(odds) > 0 and winning_class in (odds[0].get("class") or [])) else "HayÄ±r"
            
            odd_x = odds[1].get_text(strip=True) if len(odds) > 1 else None
            odd_x_won = "Evet" if (len(odds) > 1 and winning_class in (odds[1].get("class") or [])) else "HayÄ±r"
            
            odd_2 = odds[2].get_text(strip=True) if len(odds) > 2 else None
            odd_2_won = "Evet" if (len(odds) > 2 and winning_class in (odds[2].get("class") or [])) else "HayÄ±r"
            
            odd_alt = odds[3].get_text(strip=True) if len(odds) > 3 else None
            odd_alt_won = "Evet" if (len(odds) > 3 and winning_class in (odds[3].get("class") or [])) else "HayÄ±r"
            
            odd_ust = odds[4].get_text(strip=True) if len(odds) > 4 else None
            odd_ust_won = "Evet" if (len(odds) > 4 and winning_class in (odds[4].get("class") or [])) else "HayÄ±r"
            
            return CompetitionHistory(
                MaÃ§_Kodu=match_code,
                GÃ¼ncel_MaÃ§=current_match,
                Lig=league,
                Tarih=date,
                Ev_Sahibi=home_team,
                Deplasman=away_team,
                MS=score,
                Ä°Y=first_half,
                Oran_1=odd_1,
                Oran_1_Geldi=odd_1_won,
                Oran_X=odd_x,
                Oran_X_Geldi=odd_x_won,
                Oran_2=odd_2,
                Oran_2_Geldi=odd_2_won,
                Oran_Alt=odd_alt,
                Oran_Alt_Geldi=odd_alt_won,
                Oran_Ãœst=odd_ust,
                Oran_Ãœst_Geldi=odd_ust_won
            )
            
        except Exception as e:
            logger.error(f"Rekabet geÃ§miÅŸi satÄ±r parse hatasÄ±: {e}")
            return None
            
    def save_competition_history_to_csv(self, filename: str = "Rekabet_Gecmisi.csv") -> str:
        """Rekabet geÃ§miÅŸi verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.competition_history:
            logger.warning("Kaydedilecek rekabet geÃ§miÅŸi verisi yok!")
            return filepath
            
        fieldnames = list(CompetitionHistory.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
            writer.writeheader()
            writer.writerows([h.to_dict() for h in self.competition_history])
            
        logger.info(f"âœ“ {len(self.competition_history)} rekabet geÃ§miÅŸi kaydedildi: {filename}")
        return filepath
        
    def get_last_matches_for_match(self, match: MatchData) -> None:
        """Bir maÃ§ iÃ§in her iki takÄ±mÄ±n son maÃ§larÄ±nÄ± Ã§eker."""
        if not match.Ä°statistik_Link or not match.MaÃ§:
            return
            
        try:
            # Son maÃ§lar URL'si
            stats_url = f"{match.Ä°statistik_Link}/son-maclari"
            self.driver.get(stats_url)
            
            # SayfanÄ±n tam yÃ¼klenmesi iÃ§in bekle
            time.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, "lxml")
            
            # TakÄ±m isimlerini ayÄ±r
            teams = match.MaÃ§.split(" - ")
            if len(teams) != 2:
                return
                
            home_team = teams[0].strip()
            away_team = teams[1].strip()
            
            home_count = 0
            away_count = 0
            
            # Birinci takÄ±mÄ±n (ev sahibi) son maÃ§larÄ±
            first_table = soup.select_one('div[data-test-id="LastMatchesTableFirst"]')
            if first_table:
                team_name_elem = first_table.select_one('a[data-test-id="TeamLink"] span')
                team_name = team_name_elem.get_text(strip=True) if team_name_elem else home_team
                
                rows = first_table.select('tr[data-test-id="LastMatchesTable"]')
                for row in rows:
                    last_match = self.parse_last_match_row(row, match.MaÃ§_Kodu, match.MaÃ§, team_name, "Ev Sahibi")
                    if last_match:
                        self.last_matches.append(last_match)
                        home_count += 1
                        
            # Ä°kinci takÄ±mÄ±n (deplasman) son maÃ§larÄ±
            second_table = soup.select_one('div[data-test-id="LastMatchesTableSecond"]')
            if second_table:
                team_name_elem = second_table.select_one('a[data-test-id="TeamLink"] span')
                team_name = team_name_elem.get_text(strip=True) if team_name_elem else away_team
                
                rows = second_table.select('tr[data-test-id="LastMatchesTable"]')
                for row in rows:
                    last_match = self.parse_last_match_row(row, match.MaÃ§_Kodu, match.MaÃ§, team_name, "Deplasman")
                    if last_match:
                        self.last_matches.append(last_match)
                        away_count += 1
                        
            if home_count > 0 or away_count > 0:
                logger.info(f"  â””â”€ Ev: {home_count}, Deplasman: {away_count} son maÃ§")
            else:
                logger.warning(f"  â””â”€ Son maÃ§lar bulunamadÄ± (bu lig iÃ§in veri olmayabilir)")
                
        except TimeoutException:
            logger.warning(f"  â””â”€ Son maÃ§lar bulunamadÄ±")
        except Exception as e:
            logger.error(f"Son maÃ§lar hatasÄ± ({match.MaÃ§}): {e}")
            
    def parse_last_match_row(self, row: Tag, match_code: str, current_match: str, team_name: str, team_type: str) -> Optional[LastMatch]:
        """Son maÃ§ satÄ±rÄ±nÄ± parse eder."""
        try:
            # Lig bilgisi
            league_elem = row.select_one('td[data-test-id="TableBodyLeague"] span:first-child')
            league = league_elem.get_text(strip=True) if league_elem else None
            
            # Tarih bilgisi
            date_elem = row.select_one('td[data-test-id="TableBodyLeague"] span:last-child')
            date = date_elem.get_text(strip=True) if date_elem else None
            
            # Ev sahibi takÄ±m
            home_elem = row.select_one('div[data-test-id="HomeTeam"] a span')
            home_team = home_elem.get_text(strip=True) if home_elem else None
            
            # Deplasman takÄ±m
            away_elem = row.select_one('div[data-test-id="AwayTeam"] a span')
            away_team = away_elem.get_text(strip=True) if away_elem else None
            
            # MaÃ§ sonucu ve sonuÃ§ rengi
            score_btn = row.select_one('button[data-test-id="NsnButton"] span')
            score = score_btn.get_text(strip=True) if score_btn else None
            
            # SonuÃ§ (Galibiyet/MaÄŸlubiyet/Beraberlik)
            result = None
            if score_btn:
                classes = score_btn.get("class") or []
                # TakÄ±mÄ±n bu maÃ§ta ev sahibi mi deplasman mÄ± olduÄŸunu bul
                is_home = self.match_team_name(team_name, home_team) if home_team else False
                
                # Skor analizi
                if score:
                    try:
                        parts = score.replace(" ", "").split("-")
                        if len(parts) == 2:
                            home_goals = int(parts[0])
                            away_goals = int(parts[1])
                            
                            if home_goals > away_goals:
                                result = "Galibiyet" if is_home else "MaÄŸlubiyet"
                            elif home_goals < away_goals:
                                result = "MaÄŸlubiyet" if is_home else "Galibiyet"
                            else:
                                result = "Beraberlik"
                    except ValueError:
                        pass
            
            # Ä°lk yarÄ± sonucu
            first_half_elem = row.select_one('td[data-test-id="TableBodyFirstHalf"]')
            first_half = first_half_elem.get_text(strip=True) if first_half_elem else None
            
            return LastMatch(
                MaÃ§_Kodu=match_code,
                GÃ¼ncel_MaÃ§=current_match,
                TakÄ±m=team_name,
                TakÄ±m_Tipi=team_type,
                Lig=league,
                Tarih=date,
                Ev_Sahibi=home_team,
                Deplasman=away_team,
                MS=score,
                Ä°Y=first_half,
                SonuÃ§=result
            )
            
        except Exception as e:
            logger.error(f"Son maÃ§ satÄ±r parse hatasÄ±: {e}")
            return None
            
    def save_last_matches_to_csv(self, filename: str = "Son_Maclar.csv") -> str:
        """Son maÃ§lar verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.last_matches:
            logger.warning("Kaydedilecek son maÃ§ verisi yok!")
            return filepath
            
        fieldnames = list(LastMatch.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
            writer.writeheader()
            writer.writerows([m.to_dict() for m in self.last_matches])
            
        logger.info(f"âœ“ {len(self.last_matches)} son maÃ§ kaydedildi: {filename}")
        return filepath
        
    def get_referee_info_for_match(self, match: MatchData) -> None:
        """Bir maÃ§ iÃ§in hakem bilgilerini Ã§eker."""
        if not match.Ä°statistik_Link or not match.MaÃ§:
            return
            
        try:
            # Hakem bilgileri URL'si
            stats_url = f"{match.Ä°statistik_Link}/hakem-bilgileri"
            self.driver.get(stats_url)
            
            # SayfanÄ±n tam yÃ¼klenmesi iÃ§in bekle
            time.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, "lxml")
            
            # Hakem adÄ±nÄ± Ã§ek
            referee_container = soup.select_one('div[data-test-id="Referee"]')
            if not referee_container:
                logger.warning(f"  â””â”€ Hakem bilgisi bulunamadÄ±")
                return
                
            referee_name_elem = referee_container.select_one('h4.cd931950a4583aede299')
            referee_name = None
            if referee_name_elem:
                # Ä°kon ve bayrak dÄ±ÅŸÄ±ndaki metin
                referee_name = referee_name_elem.get_text(strip=True)
                
            if not referee_name:
                logger.warning(f"  â””â”€ Hakem adÄ± bulunamadÄ±")
                return
                
            # Hakem maÃ§larÄ±nÄ± Ã§ek
            match_count = 0
            history_rows = referee_container.select('div[data-test-id="CompitionHistoryTableItem"]')
            
            for row in history_rows:
                referee_match = self.parse_referee_match_row(row, match.MaÃ§_Kodu, match.MaÃ§, referee_name)
                if referee_match:
                    self.referee_matches.append(referee_match)
                    match_count += 1
                    
            # Hakem istatistiklerini Ã§ek
            stats = self.parse_referee_stats(soup, match.MaÃ§_Kodu, match.MaÃ§, referee_name)
            if stats:
                self.referee_stats.append(stats)
                
            logger.info(f"  â””â”€ {referee_name}: {match_count} maÃ§")
                
        except TimeoutException:
            logger.warning(f"  â””â”€ Hakem bilgileri bulunamadÄ±")
        except Exception as e:
            logger.error(f"Hakem bilgileri hatasÄ± ({match.MaÃ§}): {e}")
            
    def parse_referee_match_row(self, row: Tag, match_code: str, current_match: str, referee_name: str) -> Optional[RefereeMatch]:
        """Hakem maÃ§ satÄ±rÄ±nÄ± parse eder."""
        try:
            # Lig bilgisi
            league_elem = row.select_one('span[data-test-id="CompitionTableItemLeague"] span:first-child')
            league = league_elem.get_text(strip=True) if league_elem else None
            
            # Tarih bilgisi
            date_elem = row.select_one('span[data-test-id="CompitionTableItemSeason"]')
            date = date_elem.get_text(strip=True) if date_elem else None
            
            # Ev sahibi takÄ±m
            home_elem = row.select_one('div[data-test-id="HomeTeam"] a span')
            home_team = home_elem.get_text(strip=True) if home_elem else None
            
            # Deplasman takÄ±m
            away_elem = row.select_one('div[data-test-id="AwayTeam"] a span')
            away_team = away_elem.get_text(strip=True) if away_elem else None
            
            # MaÃ§ sonucu
            score_elem = row.select_one('button[data-test-id="NsnButton"] span')
            score = score_elem.get_text(strip=True) if score_elem else None
            
            # Ä°lk yarÄ± sonucu
            first_half_elem = row.select_one('span[data-test-id="CompitionTableItemFirstHalf"]')
            first_half = first_half_elem.get_text(strip=True) if first_half_elem else None
            
            # Oranlar
            odds_container = row.select_one('div[data-test-id="CompitionTableItemOdds"]')
            odds = odds_container.select('span[data-test-id="CompitionHistoryTableItem"]') if odds_container else []
            
            # Kazanan oran class'Ä±
            winning_class = "ab18fc768d1ec03e3ada"
            
            # OranlarÄ± parse et
            odd_1 = odds[0].get_text(strip=True) if len(odds) > 0 else None
            odd_1_won = "Evet" if (len(odds) > 0 and winning_class in (odds[0].get("class") or [])) else "HayÄ±r"
            
            odd_x = odds[1].get_text(strip=True) if len(odds) > 1 else None
            odd_x_won = "Evet" if (len(odds) > 1 and winning_class in (odds[1].get("class") or [])) else "HayÄ±r"
            
            odd_2 = odds[2].get_text(strip=True) if len(odds) > 2 else None
            odd_2_won = "Evet" if (len(odds) > 2 and winning_class in (odds[2].get("class") or [])) else "HayÄ±r"
            
            odd_alt = odds[3].get_text(strip=True) if len(odds) > 3 else None
            odd_alt_won = "Evet" if (len(odds) > 3 and winning_class in (odds[3].get("class") or [])) else "HayÄ±r"
            
            odd_ust = odds[4].get_text(strip=True) if len(odds) > 4 else None
            odd_ust_won = "Evet" if (len(odds) > 4 and winning_class in (odds[4].get("class") or [])) else "HayÄ±r"
            
            return RefereeMatch(
                MaÃ§_Kodu=match_code,
                GÃ¼ncel_MaÃ§=current_match,
                Hakem_AdÄ±=referee_name,
                Lig=league,
                Tarih=date,
                Ev_Sahibi=home_team,
                Deplasman=away_team,
                MS=score,
                Ä°Y=first_half,
                Oran_1=odd_1,
                Oran_1_Geldi=odd_1_won,
                Oran_X=odd_x,
                Oran_X_Geldi=odd_x_won,
                Oran_2=odd_2,
                Oran_2_Geldi=odd_2_won,
                Oran_Alt=odd_alt,
                Oran_Alt_Geldi=odd_alt_won,
                Oran_Ãœst=odd_ust,
                Oran_Ãœst_Geldi=odd_ust_won
            )
            
        except Exception as e:
            logger.error(f"Hakem maÃ§ satÄ±r parse hatasÄ±: {e}")
            return None
            
    def parse_referee_stats(self, soup: BeautifulSoup, match_code: str, current_match: str, referee_name: str) -> Optional[RefereeStats]:
        """Hakem istatistiklerini parse eder."""
        try:
            stats_container = soup.select_one('div[data-test-id="setContent"]')
            if not stats_container:
                return None
                
            # Ä°statistik deÄŸerlerini Ã§ek
            stats_items = soup.select('div[data-test-id="TableItem"]')
            
            stats_dict = {}
            for item in stats_items:
                label_elem = item.select_one('div.f15d176d9b8eb47234b0 span:first-child')
                value_elem = item.select_one('div.f15d176d9b8eb47234b0 span:last-child')
                percent_elem = item.select_one('span.c411ef110bae2cf448ba span')
                
                if label_elem and value_elem:
                    label = label_elem.get_text(strip=True)
                    value = value_elem.get_text(strip=True)
                    percent = percent_elem.get_text(strip=True) if percent_elem else None
                    stats_dict[label] = {"count": value, "percent": percent}
                    
            return RefereeStats(
                MaÃ§_Kodu=match_code,
                GÃ¼ncel_MaÃ§=current_match,
                Hakem_AdÄ±=referee_name,
                MS1_SayÄ±=stats_dict.get("MS1", {}).get("count"),
                MS1_YÃ¼zde=stats_dict.get("MS1", {}).get("percent"),
                MSX_SayÄ±=stats_dict.get("MSX", {}).get("count"),
                MSX_YÃ¼zde=stats_dict.get("MSX", {}).get("percent"),
                MS2_SayÄ±=stats_dict.get("MS2", {}).get("count"),
                MS2_YÃ¼zde=stats_dict.get("MS2", {}).get("percent"),
                Alt_2_5_SayÄ±=stats_dict.get("2,5 Alt", {}).get("count"),
                Alt_2_5_YÃ¼zde=stats_dict.get("2,5 Alt", {}).get("percent"),
                Ãœst_2_5_SayÄ±=stats_dict.get("2,5 Ãœst", {}).get("count"),
                Ãœst_2_5_YÃ¼zde=stats_dict.get("2,5 Ãœst", {}).get("percent"),
                KG_Var_SayÄ±=stats_dict.get("KG Var", {}).get("count"),
                KG_Var_YÃ¼zde=stats_dict.get("KG Var", {}).get("percent"),
                KG_Yok_SayÄ±=stats_dict.get("KG Yok", {}).get("count"),
                KG_Yok_YÃ¼zde=stats_dict.get("KG Yok", {}).get("percent")
            )
            
        except Exception as e:
            logger.error(f"Hakem istatistik parse hatasÄ±: {e}")
            return None
            
    def save_referee_matches_to_csv(self, filename: str = "Hakem_Bilgileri.csv") -> str:
        """Hakem maÃ§ verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.referee_matches:
            logger.warning("Kaydedilecek hakem maÃ§ verisi yok!")
            return filepath
            
        fieldnames = list(RefereeMatch.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
            writer.writeheader()
            writer.writerows([m.to_dict() for m in self.referee_matches])
            
        logger.info(f"âœ“ {len(self.referee_matches)} hakem maÃ§Ä± kaydedildi: {filename}")
        return filepath
        
    def save_referee_stats_to_csv(self, filename: str = "Hakem_Istatistikleri.csv") -> str:
        """Hakem istatistik verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.referee_stats:
            logger.warning("Kaydedilecek hakem istatistik verisi yok!")
            return filepath
            
        fieldnames = list(RefereeStats.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=";")
            writer.writeheader()
            writer.writerows([s.to_dict() for s in self.referee_stats])
            
        logger.info(f"âœ“ {len(self.referee_stats)} hakem istatistiÄŸi kaydedildi: {filename}")
        return filepath
        
    def get_injury_data_for_match(self, match: MatchData) -> None:
        """Bir maÃ§ iÃ§in sakat ve cezalÄ± oyuncu verilerini Ã§eker."""
        if not match.Ä°statistik_Link or not match.MaÃ§:
            return
            
        try:
            # Sakat/CezalÄ± URL'si
            stats_url = f"{match.Ä°statistik_Link}/sakat-cezali"
            self.driver.get(stats_url)
            
            # SayfanÄ±n tam yÃ¼klenmesi iÃ§in bekle
            time.sleep(2)
            
            soup = BeautifulSoup(self.driver.page_source, "lxml")
            
            # Sakat/CezalÄ± ana container'Ä± bul
            injury_container = soup.select_one('div[data-test-id="CrippledPunished"]')
            if not injury_container:
                logger.info(f"  â””â”€ Sakat/cezalÄ± verisi yok")
                return
                
            # Her iki takÄ±m iÃ§in ayrÄ± ayrÄ± verileri Ã§ek
            team_containers = injury_container.select('div.ad65c734cbc1c4292120')
            
            total_count = 0
            for team_container in team_containers:
                # TakÄ±m adÄ±nÄ± Ã§ek
                team_link = team_container.select_one('a[data-test-id="TeamLink"] span')
                team_name = team_link.get_text(strip=True) if team_link else None
                
                if not team_name:
                    continue
                    
                # Oyuncu satÄ±rlarÄ±nÄ± Ã§ek
                player_rows = team_container.select('div[data-test-id="MissingPlayersTable"]')
                
                for row in player_rows:
                    injury = self.parse_injury_player_row(row, match.MaÃ§_Kodu, match.MaÃ§, team_name)
                    if injury:
                        self.injury_data.append(injury)
                        total_count += 1
                        
            if total_count > 0:
                logger.info(f"  â””â”€ {total_count} sakat/cezalÄ± oyuncu bulundu")
            else:
                logger.info(f"  â””â”€ Sakat/cezalÄ± oyuncu yok")
                
        except TimeoutException:
            logger.warning(f"  â””â”€ Sakat/cezalÄ± sayfasÄ± yÃ¼klenemedi")
        except Exception as e:
            logger.error(f"Sakat/cezalÄ± hatasÄ± ({match.MaÃ§}): {e}")
            
    def parse_injury_player_row(self, row: Tag, match_code: str, current_match: str, team_name: str) -> Optional[InjuryData]:
        """Sakat/cezalÄ± oyuncu satÄ±rÄ±nÄ± parse eder."""
        try:
            # Forma numarasÄ±
            number_elem = row.select_one('span[data-test-id="Number"] span')
            number = number_elem.get_text(strip=True) if number_elem else None
            
            # Oyuncu adÄ±
            player_elem = row.select_one('span[data-test-id="Player"] a')
            player_name = player_elem.get_text(strip=True) if player_elem else None
            
            if not player_name:
                return None
                
            # YaÅŸ
            age_elem = row.select_one('span[data-test-id="Age"]')
            age = age_elem.get_text(strip=True) if age_elem else None
            
            # Pozisyon
            position_elem = row.select_one('span[data-test-id="Position"]')
            position = position_elem.get_text(strip=True) if position_elem else None
            
            # MaÃ§ sayÄ±sÄ±
            match_count_elem = row.select_one('span[data-test-id="Match"]')
            match_count = match_count_elem.get_text(strip=True) if match_count_elem else "0"
            
            # Ä°lk 11
            first_eleven_elem = row.select_one('span[data-test-id="FirstEleven"]')
            first_eleven = first_eleven_elem.get_text(strip=True) if first_eleven_elem else "0"
            # "-" iÅŸaretini 0'a Ã§evir
            if first_eleven == "-":
                first_eleven = "0"
                
            # Gol
            goal_elem = row.select_one('span[data-test-id="Goal"]')
            goal = goal_elem.get_text(strip=True) if goal_elem else "0"
            if goal == "-":
                goal = "0"
                
            # Asist
            assist_elem = row.select_one('span[data-test-id="Assist"]')
            assist = assist_elem.get_text(strip=True) if assist_elem else "0"
            if assist == "-":
                assist = "0"
                
            # Durum ve AÃ§Ä±klama
            description_elem = row.select_one('span[data-test-id="Description"] span')
            full_description = description_elem.get_text(strip=True) if description_elem else None
            
            # Durum ve aÃ§Ä±klamayÄ± ayÄ±r
            status = None
            description = None
            if full_description:
                if "SakatlÄ±k" in full_description:
                    status = "SakatlÄ±k"
                    # "SakatlÄ±k - " kÄ±smÄ±nÄ± Ã§Ä±kar
                    description = full_description.replace("SakatlÄ±k - ", "").strip()
                elif "CezalÄ±" in full_description:
                    status = "CezalÄ±"
                    # "CezalÄ± - " kÄ±smÄ±nÄ± Ã§Ä±kar
                    description = full_description.replace("CezalÄ± - ", "").strip()
                else:
                    status = "Bilinmiyor"
                    description = full_description
                    
            return InjuryData(
                MaÃ§_Kodu=match_code,
                MaÃ§=current_match,
                TakÄ±m=team_name,
                Numara=number,
                Oyuncu=player_name,
                YaÅŸ=age,
                Pozisyon=position,
                MaÃ§_SayÄ±sÄ±=match_count,
                Ä°lk_11=first_eleven,
                Gol=goal,
                Asist=assist,
                Durum=status,
                AÃ§Ä±klama=description
            )
            
        except Exception as e:
            logger.error(f"Sakat/cezalÄ± oyuncu parse hatasÄ±: {e}")
            return None
            
    def save_injury_data_to_csv(self, filename: str = "Sakat_Cezali.csv") -> str:
        """Sakat/cezalÄ± oyuncu verilerini CSV dosyasÄ±na kaydeder."""
        filepath = os.path.join(os.path.dirname(os.path.abspath(__file__)), filename)
        
        if not self.injury_data:
            logger.warning("Kaydedilecek sakat/cezalÄ± verisi yok!")
            return filepath
            
        fieldnames = list(InjuryData.__dataclass_fields__.keys())
        
        with open(filepath, "w", newline="", encoding="utf-8-sig") as f:
            writer = csv.DictWriter(f, fieldnames=fieldnames, delimiter=",")
            writer.writeheader()
            writer.writerows([i.to_dict() for i in self.injury_data])
            
        logger.info(f"âœ“ {len(self.injury_data)} sakat/cezalÄ± oyuncu kaydedildi: {filename}")
        return filepath
        
    def run(self) -> None:
        """Ana Ã§alÄ±ÅŸtÄ±rma fonksiyonu."""
        try:
            logger.info("Nesine.com BÃ¼lten & Ä°statistik Scraper BaÅŸlatÄ±lÄ±yor...")
            logger.info("="*60)
            
            self.setup_driver()
            
            # URL'yi parametrelerle oluÅŸtur
            params = "&".join(f"{k}={v}" for k, v in self.DEFAULT_PARAMS.items())
            url = f"{self.BASE_URL}?{params}"
            logger.info(f"URL: {url}")
            self.driver.get(url)
            
            self.wait_for_page_load()
            self.close_popups()

            # â”€â”€ Agresif Scroll + Incremental Collection â”€â”€
            # Virtualized DOM'da tek page_source tÃ¼m verileri iÃ§ermeyebilir.
            # Bu yÃ¼zden scroll sÄ±rasÄ±nda periyodik olarak veri topluyoruz.
            self._scroll_and_collect()

            # Eksik kalan varsa son bir deneme daha yap
            if len(self.matches) < self.match_count:
                logger.info(
                    f"Ä°nkremental toplamada {len(self.matches)}/{self.match_count}, "
                    f"son page_source ile tamamlanmaya Ã§alÄ±ÅŸÄ±lÄ±yor..."
                )
                soup = self.get_page_source()
                self.get_match_data(soup)

            logger.info(f"ğŸ“‹ Toplam {len(self.matches)} maÃ§ toplandÄ±")
            
            # BÃ¼lten verilerini kaydet
            self.save_matches_to_csv()
            
            # Puan tablosu verilerini Ã§ek
            logger.info("="*60)
            logger.info("Puan tablosu verileri Ã§ekiliyor...")
            logger.info("-"*60)
            
            for i, match in enumerate(self.matches, 1):
                logger.info(f"[{i}/{len(self.matches)}] {match.MaÃ§}")
                self.get_standings_for_match(match)
                
            # Puan tablosu verilerini kaydet
            logger.info("="*60)
            self.save_standings_to_csv()
            
            # Rekabet geÃ§miÅŸi verilerini Ã§ek
            logger.info("="*60)
            logger.info("Rekabet geÃ§miÅŸi verileri Ã§ekiliyor...")
            logger.info("-"*60)
            
            for i, match in enumerate(self.matches, 1):
                logger.info(f"[{i}/{len(self.matches)}] {match.MaÃ§}")
                self.get_competition_history_for_match(match)
                
            # Rekabet geÃ§miÅŸi verilerini kaydet
            logger.info("="*60)
            self.save_competition_history_to_csv()
            
            # Son maÃ§lar verilerini Ã§ek
            logger.info("="*60)
            logger.info("Son maÃ§lar verileri Ã§ekiliyor...")
            logger.info("-"*60)
            
            for i, match in enumerate(self.matches, 1):
                logger.info(f"[{i}/{len(self.matches)}] {match.MaÃ§}")
                self.get_last_matches_for_match(match)
                
            # Son maÃ§lar verilerini kaydet
            logger.info("="*60)
            self.save_last_matches_to_csv()
            
            # Hakem bilgileri verilerini Ã§ek
            logger.info("="*60)
            logger.info("Hakem bilgileri Ã§ekiliyor...")
            logger.info("-"*60)
            
            for i, match in enumerate(self.matches, 1):
                logger.info(f"[{i}/{len(self.matches)}] {match.MaÃ§}")
                self.get_referee_info_for_match(match)
                
            # Hakem verilerini kaydet
            logger.info("="*60)
            self.save_referee_matches_to_csv()
            self.save_referee_stats_to_csv()
            
            # Sakat/CezalÄ± oyuncu verilerini Ã§ek
            logger.info("="*60)
            logger.info("Sakat/CezalÄ± oyuncu verileri Ã§ekiliyor...")
            logger.info("-"*60)
            
            for i, match in enumerate(self.matches, 1):
                logger.info(f"[{i}/{len(self.matches)}] {match.MaÃ§}")
                self.get_injury_data_for_match(match)
                
            # Sakat/CezalÄ± verilerini kaydet
            logger.info("="*60)
            self.save_injury_data_to_csv()
            
            logger.info("="*60)
            logger.info("âœ“ TÃ¼m iÅŸlemler tamamlandÄ±!")
            logger.info("="*60)
            
        except Exception as e:
            logger.error(f"Kritik hata: {e}")
            raise
            
        finally:
            if self.driver:
                self.driver.quit()
                logger.info("WebDriver kapatÄ±ldÄ±")


def main() -> None:
    """Ana giriÅŸ noktasÄ±."""
    try:
        match_input = input("KaÃ§ adet maÃ§ Ã§ekmek istiyorsunuz? ")
        match_count = int(match_input)
        
        if match_count <= 0:
            logger.error("GeÃ§ersiz sayÄ±! En az 1 maÃ§ girmelisiniz.")
            return
            
        scraper = NesineScraper(match_count=match_count)
        scraper.run()
        
    except ValueError:
        logger.error("LÃ¼tfen geÃ§erli bir sayÄ± girin!")
    except KeyboardInterrupt:
        logger.info("Ä°ÅŸlem kullanÄ±cÄ± tarafÄ±ndan iptal edildi.")


if __name__ == "__main__":
    main()
