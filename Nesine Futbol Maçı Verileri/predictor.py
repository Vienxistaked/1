"""
ğŸ¤– Hibrit Tahmin Motoru v3.1 (MatchPredictor â€” Stacking Ensemble)
Poisson + Stacking Ensemble (CatBoost + LightGBM + XGBoost â†’ LogisticRegression)
ile maÃ§ sonucu tahmini + SHAP Explainability.

Mimari:
  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
  â”‚             Layer 1 (Base Learners)                â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
  â”‚  â”‚  CatBoost â”‚ â”‚ LightGBM  â”‚ â”‚   XGBoost     â”‚    â”‚
  â”‚  â”‚ (96 feat) â”‚ â”‚ (96 feat) â”‚ â”‚  (96 feat)    â”‚    â”‚
  â”‚  â”‚ +4 categ. â”‚ â”‚           â”‚ â”‚               â”‚    â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
  â”‚        â”‚              â”‚              â”‚             â”‚
  â”‚        â–¼              â–¼              â–¼             â”‚
  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
  â”‚  â”‚   Layer 2 (Meta-Learner)                    â”‚   â”‚
  â”‚  â”‚   LogisticRegression(C=1.0)                 â”‚   â”‚
  â”‚  â”‚   Input: 9 class probabilities (3Ã—3)        â”‚   â”‚
  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
  â”‚                        â”‚                           â”‚
  â”‚                        â–¼                           â”‚
  â”‚              Final Prediction (1/X/2)              â”‚
  â”‚              + SHAP Feature Importance             â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Cold-Start MekanizmasÄ±:
  â€¢ < 50  maÃ§  â†’ Saf Poisson
  â€¢ 50â€“200 maÃ§ â†’ Poisson (%60) + Stacking (%40)  hibrit
  â€¢ > 200 maÃ§  â†’ Stacking aÄŸÄ±rlÄ±klÄ± (%70) + Poisson (%30)

v3.1 DeÄŸiÅŸiklikleri (v3.0 Ã¼zerinden):
  âœ“ Bayesian Smoothing: Erken sezon overfitting Ã§Ã¶zÃ¼mÃ¼
    â€” Puan tablosu feature'larÄ± lig medyanÄ±na Ã§ekilir (dampened_rank)
    â€” Poisson Î» hesaplamasÄ± Bayesian Average ile sÃ¶nÃ¼mlenir
    â€” season_progress, season_confidence ile sezon konteksti
  âœ“ Dynamic Feature Trust: Erken sezonda implied_prob aÄŸÄ±rlÄ±ÄŸÄ± artar,
    standing feature aÄŸÄ±rlÄ±ÄŸÄ± azalÄ±r (season_confidence Ã¼zerinden)
  âœ“ Ä°lgili yeni feature'lar: relative_market_strength, early_season_reliability
  âœ“ Risk deÄŸerlendirme: Erken sezon gÃ¼venilirlik eksikliÄŸi faktÃ¶rÃ¼ eklendi
  âœ“ 96-feature vektÃ¶rÃ¼ (feature_engineering v3.1, 11 yeni feature)

v3.0 DeÄŸiÅŸiklikleri (v2.1 Ã¼zerinden):
  âœ“ XGBoost tek model â†’ Stacking Ensemble (CatBoost+LightGBM+XGBoost)
  âœ“ LogisticRegression meta-learner (Layer 2)
  âœ“ CatBoost native categorical feature desteÄŸi (4 kategorik: takÄ±m+hakem)
  âœ“ 85-feature vektÃ¶rÃ¼ (feature_engineering v3.0)
  âœ“ SHAP entegrasyonu â†’ "Neden MS1?" human-readable aÃ§Ä±klamalar
  âœ“ Kronolojik 5-Fold CV ile stacking-oof eÄŸitimi (data leakage korumasÄ±)
  âœ“ MODEL_VERSION = "v3.1" + otomatik cache invalidation
"""

from __future__ import annotations

import hashlib
import logging
import pickle
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import TimeSeriesSplit
from sklearn.metrics import log_loss
from sqlalchemy.orm import Session

try:
    import optuna
    optuna.logging.set_verbosity(optuna.logging.WARNING)
    HAS_OPTUNA = True
except ImportError:
    HAS_OPTUNA = False

from config import (
    MIN_TRAINING_SAMPLES,
    MIN_TRAINING_SAMPLES_XGBOOST,
    MODEL_DIR,
    RANDOM_SEED,
    VALUE_BET_MIN_EDGE,
    VALUE_BET_MIN_CONFIDENCE,
)
from models import Match, Odds, Prediction
from feature_engineering import (
    FeatureExtractor,
    build_training_dataset,
    build_training_dataset_with_categorical,
)
from poisson_model import PoissonModel, PoissonResult

logger = logging.getLogger(__name__)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  Tahmin Sonucu Veri SÄ±nÄ±fÄ±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
@dataclass
class PredictionResult:
    """Tek bir maÃ§ iÃ§in birleÅŸik tahmin sonucu."""

    match_id: int
    match_display: str
    engine_used: str          # "poisson" | "ml" | "hybrid"
    model_version: str

    # OlasÄ±lÄ±klar (%)
    prob_home: float = 0.0
    prob_draw: float = 0.0
    prob_away: float = 0.0
    prob_over_25: float = 0.0
    prob_under_25: float = 0.0

    # Poisson beklentileri
    expected_home_goals: float = 0.0
    expected_away_goals: float = 0.0
    top_scores: List[Tuple[str, float]] = field(default_factory=list)

    # Final tahmin
    prediction: str = ""       # "1" | "X" | "2"
    confidence: float = 0.0
    value_edge: float = 0.0
    is_value_bet: bool = False
    risk_level: str = ""

    explanation: str = ""

    # v3.0: SHAP bilgileri
    shap_top_features: List[Tuple[str, float]] = field(default_factory=list)
    shap_summary: str = ""

    def to_prediction_model(self) -> Dict[str, object]:
        """``Prediction`` ORM modeli iÃ§in dict dÃ¶ndÃ¼rÃ¼r."""
        return {
            "engine_used": self.engine_used,
            "model_version": self.model_version,
            "prob_home": self.prob_home,
            "prob_draw": self.prob_draw,
            "prob_away": self.prob_away,
            "prob_over_25": self.prob_over_25,
            "prob_under_25": self.prob_under_25,
            "expected_home_goals": self.expected_home_goals,
            "expected_away_goals": self.expected_away_goals,
            "prediction": self.prediction,
            "confidence": self.confidence,
            "value_edge": self.value_edge,
            "is_value_bet": self.is_value_bet,
            "risk_level": self.risk_level,
            "explanation": self.explanation,
        }


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  Stacking Ensemble SÄ±nÄ±fÄ±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class StackingEnsemble:
    """Layer 1: CatBoost + LightGBM + XGBoost
    Layer 2: LogisticRegression meta-learner

    Temporal (kronolojik) K-Fold CV ile out-of-fold (OOF) tahminler Ã¼retir.
    Meta-learner OOF olasÄ±lÄ±klarÄ±nÄ± girdi olarak alÄ±p final tahmin yapar.
    """

    def __init__(self, use_optuna: bool = True, optuna_n_trials: int = 30) -> None:
        self.base_models: List[Tuple[str, Any]] = []
        self.meta_model: Optional[LogisticRegression] = None
        self.is_fitted: bool = False
        self._has_catboost: bool = False
        self._has_lightgbm: bool = False
        self._has_xgboost: bool = False
        self._cat_feature_indices: List[int] = []
        self._use_optuna: bool = use_optuna and HAS_OPTUNA
        self._optuna_n_trials: int = optuna_n_trials
        self._best_params: Dict[str, Dict[str, Any]] = {}

    def fit(
        self,
        X: np.ndarray,
        y: np.ndarray,
        cat_features: Optional[List[Dict[str, str]]] = None,
        n_splits: int = 5,
    ) -> Dict[str, float]:
        """Stacking modelini eÄŸitir.

        Parameters
        ----------
        X : np.ndarray  (n_samples, 96)
        y : np.ndarray  (n_samples,) â€” 0/1/2 labels
        cat_features : CatBoost iÃ§in kategorik feature listesi
        n_splits : Temporal CV split sayÄ±sÄ±

        Returns
        -------
        Dict[str, float]
            Her base model ve final ensemble doÄŸruluÄŸu.
        """
        X = np.nan_to_num(X, nan=0.0)
        n_classes = len(np.unique(y))

        # â”€â”€ Optuna HPO (opsiyonel) â”€â”€
        if self._use_optuna and len(X) >= 100:
            logger.info("ğŸ”¬ Optuna HPO baÅŸlatÄ±lÄ±yor (%d trial)...", self._optuna_n_trials)
            self._best_params = self.optimize_hyperparameters(
                X, y, n_trials=self._optuna_n_trials, n_splits=n_splits,
            )
            logger.info("âœ“ Optuna HPO tamamlandÄ±: %s", list(self._best_params.keys()))

        # â”€â”€ Base modelleri hazÄ±rla â”€â”€
        self.base_models = []
        self._init_base_models(cat_features)

        if not self.base_models:
            raise RuntimeError("HiÃ§bir base model yÃ¼klenemedi!")

        n_base = len(self.base_models)
        logger.info(
            "ğŸ”§ Stacking eÄŸitimi baÅŸlÄ±yor: %d base model, %d split",
            n_base, n_splits,
        )

        # â”€â”€ Temporal K-Fold â†’ OOF tahminler â”€â”€
        tscv = TimeSeriesSplit(n_splits=n_splits)
        oof_preds = np.zeros((len(X), n_base * n_classes))
        oof_mask = np.zeros(len(X), dtype=bool)  # Hangi satÄ±rlara OOF yazÄ±ldÄ±

        accuracies: Dict[str, List[float]] = {name: [] for name, _ in self.base_models}

        for fold_idx, (train_idx, val_idx) in enumerate(tscv.split(X)):
            X_tr, X_val = X[train_idx], X[val_idx]
            y_tr, y_val = y[train_idx], y[val_idx]

            for model_idx, (name, model) in enumerate(self.base_models):
                try:
                    if name == "CatBoost" and self._cat_feature_indices and cat_features:
                        # CatBoost native categorical â†’ full matrix (numeric + encoded cat)
                        X_tr_cat, X_val_cat = self._prepare_catboost_data(
                            X_tr, X_val, cat_features, train_idx, val_idx,
                        )
                        model.fit(
                            X_tr_cat, y_tr,
                            eval_set=(X_val_cat, y_val),
                            verbose=0,
                        )
                        probs = model.predict_proba(X_val_cat)
                    else:
                        model.fit(X_tr, y_tr)
                        probs = model.predict_proba(X_val)

                    # OOF olasÄ±lÄ±klarÄ±nÄ± yaz
                    start_col = model_idx * n_classes
                    end_col = start_col + n_classes
                    oof_preds[val_idx, start_col:end_col] = probs

                    # Fold doÄŸruluÄŸu
                    fold_preds = np.argmax(probs, axis=1)
                    fold_acc = np.mean(fold_preds == y_val)
                    accuracies[name].append(fold_acc)

                except Exception as e:
                    logger.warning(
                        "Fold %d / %s hatasÄ±: %s", fold_idx, name, e,
                    )
                    start_col = model_idx * n_classes
                    end_col = start_col + n_classes
                    # EÅŸit olasÄ±lÄ±k fallback
                    oof_preds[val_idx, start_col:end_col] = 1.0 / n_classes

            oof_mask[val_idx] = True

        # â”€â”€ Son tÃ¼m veri Ã¼zerinde base modelleri yeniden eÄŸit â”€â”€
        for name, model in self.base_models:
            try:
                if name == "CatBoost" and self._cat_feature_indices and cat_features:
                    X_full_cat = self._prepare_catboost_full(X, cat_features)
                    model.fit(X_full_cat, y, verbose=0)
                else:
                    model.fit(X, y)
            except Exception as e:
                logger.warning("Final fit hatasÄ± (%s): %s", name, e)

        # â”€â”€ Meta-learner (Layer 2) eÄŸitimi â”€â”€
        X_meta = oof_preds[oof_mask]
        y_meta = y[oof_mask]

        self.meta_model = LogisticRegression(
            C=1.0,
            max_iter=1000,
            multi_class="multinomial",
            solver="lbfgs",
            random_state=RANDOM_SEED,
        )
        self.meta_model.fit(X_meta, y_meta)

        self.is_fitted = True

        # â”€â”€ Final doÄŸruluk hesapla â”€â”€
        meta_preds = self.meta_model.predict(X_meta)
        meta_acc = float(np.mean(meta_preds == y_meta))

        result: Dict[str, float] = {
            "stacking_accuracy": meta_acc,
        }
        for name, acc_list in accuracies.items():
            result[f"{name}_avg_accuracy"] = float(np.mean(acc_list)) if acc_list else 0.0

        logger.info("âœ“ Stacking doÄŸruluÄŸu: %.2f%%", meta_acc * 100)
        for name, avg in result.items():
            if name != "stacking_accuracy":
                logger.info("  â€¢ %s: %.2f%%", name, avg * 100)

        return result

    def predict_proba(self, X: np.ndarray) -> np.ndarray:
        """Stacking ensemble ile olasÄ±lÄ±k tahmini.

        Returns
        -------
        np.ndarray  shape (n_samples, 3) â€” [P(1), P(X), P(2)]
        """
        if not self.is_fitted or self.meta_model is None:
            raise RuntimeError("Model henÃ¼z eÄŸitilmedi!")

        X = np.nan_to_num(X.reshape(1, -1) if X.ndim == 1 else X, nan=0.0)
        n_classes = 3
        n_base = len(self.base_models)
        meta_input = np.zeros((len(X), n_base * n_classes))

        for model_idx, (name, model) in enumerate(self.base_models):
            try:
                probs = model.predict_proba(X)
                start_col = model_idx * n_classes
                end_col = start_col + n_classes
                meta_input[:, start_col:end_col] = probs
            except Exception as e:
                logger.warning("Predict hatasÄ± (%s): %s", name, e)
                start_col = model_idx * n_classes
                end_col = start_col + n_classes
                meta_input[:, start_col:end_col] = 1.0 / n_classes

        return self.meta_model.predict_proba(meta_input)

    def predict_proba_catboost(
        self,
        X_numeric: np.ndarray,
        cat_dict: Optional[Dict[str, str]] = None,
    ) -> np.ndarray:
        """CatBoost kategorik feature destekli tahmin.

        CatBoost'a kategorik feature'lar ayrÄ±ca verilir,
        diÄŸer modeller sadece numeric alÄ±r.
        """
        if not self.is_fitted or self.meta_model is None:
            raise RuntimeError("Model henÃ¼z eÄŸitilmedi!")

        X = np.nan_to_num(
            X_numeric.reshape(1, -1) if X_numeric.ndim == 1 else X_numeric,
            nan=0.0,
        )
        n_classes = 3
        n_base = len(self.base_models)
        meta_input = np.zeros((len(X), n_base * n_classes))

        for model_idx, (name, model) in enumerate(self.base_models):
            try:
                if name == "CatBoost" and cat_dict and self._has_catboost:
                    # CatBoost prediction doesn't need cat features if trained with indices
                    probs = model.predict_proba(X)
                else:
                    probs = model.predict_proba(X)

                start_col = model_idx * n_classes
                end_col = start_col + n_classes
                meta_input[:, start_col:end_col] = probs
            except Exception as e:
                logger.warning("Predict hatasÄ± (%s): %s", name, e)
                start_col = model_idx * n_classes
                end_col = start_col + n_classes
                meta_input[:, start_col:end_col] = 1.0 / n_classes

        return self.meta_model.predict_proba(meta_input)

    # â”€â”€â”€ Dahili: Base modelleri oluÅŸtur â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    # â”€â”€â”€ Optuna Hiperparametre Optimizasyonu â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def optimize_hyperparameters(
        self,
        X: np.ndarray,
        y: np.ndarray,
        n_trials: int = 30,
        n_splits: int = 3,
    ) -> Dict[str, Dict[str, Any]]:
        """Optuna ile CatBoost, LightGBM ve XGBoost hiperparametre optimizasyonu.

        Parameters
        ----------
        X : np.ndarray â€” eÄŸitim verisi
        y : np.ndarray â€” etiketler (0/1/2)
        n_trials : Optuna deneme sayÄ±sÄ±
        n_splits : TimeSeriesSplit sayÄ±sÄ±

        Returns
        -------
        Dict[str, Dict[str, Any]]
            Her model iÃ§in en iyi hiperparametreler.
        """
        if not HAS_OPTUNA:
            logger.warning("âš  Optuna yÃ¼klÃ¼ deÄŸil, varsayÄ±lan parametreler kullanÄ±lacak")
            return {}

        best_params: Dict[str, Dict[str, Any]] = {}
        tscv = TimeSeriesSplit(n_splits=min(n_splits, max(2, len(X) // 50)))

        # â”€â”€ CatBoost HPO â”€â”€
        try:
            from catboost import CatBoostClassifier

            def _catboost_objective(trial: optuna.Trial) -> float:
                params = {
                    "iterations": trial.suggest_int("iterations", 100, 500),
                    "depth": trial.suggest_int("depth", 4, 10),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
                    "l2_leaf_reg": trial.suggest_float("l2_leaf_reg", 1e-2, 10.0, log=True),
                    "bagging_temperature": trial.suggest_float("bagging_temperature", 0.0, 1.0),
                    "loss_function": "MultiClass",
                    "eval_metric": "MultiClass",
                    "random_seed": RANDOM_SEED,
                    "verbose": 0,
                    "early_stopping_rounds": 30,
                    "use_best_model": True,
                }
                scores = []
                for train_idx, val_idx in tscv.split(X):
                    model = CatBoostClassifier(**params)
                    model.fit(X[train_idx], y[train_idx],
                              eval_set=(X[val_idx], y[val_idx]), verbose=0)
                    probs = model.predict_proba(X[val_idx])
                    scores.append(log_loss(y[val_idx], probs))
                return float(np.mean(scores))

            study = optuna.create_study(direction="minimize", study_name="catboost_hpo")
            study.optimize(_catboost_objective, n_trials=n_trials, show_progress_bar=False)
            best_params["CatBoost"] = study.best_params
            logger.info("âœ“ CatBoost HPO tamamlandÄ± (logloss: %.4f)", study.best_value)
        except ImportError:
            pass
        except Exception as e:
            logger.warning("âš  CatBoost HPO hatasÄ±: %s", e)

        # â”€â”€ LightGBM HPO â”€â”€
        try:
            from lightgbm import LGBMClassifier

            def _lgbm_objective(trial: optuna.Trial) -> float:
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 100, 500),
                    "max_depth": trial.suggest_int("max_depth", 3, 10),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
                    "num_leaves": trial.suggest_int("num_leaves", 15, 63),
                    "subsample": trial.suggest_float("subsample", 0.5, 1.0),
                    "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                    "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 10.0, log=True),
                    "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
                    "min_child_samples": trial.suggest_int("min_child_samples", 5, 50),
                    "objective": "multiclass",
                    "num_class": 3,
                    "random_state": RANDOM_SEED,
                    "verbose": -1,
                    "n_jobs": -1,
                }
                scores = []
                for train_idx, val_idx in tscv.split(X):
                    model = LGBMClassifier(**params)
                    model.fit(X[train_idx], y[train_idx])
                    probs = model.predict_proba(X[val_idx])
                    scores.append(log_loss(y[val_idx], probs))
                return float(np.mean(scores))

            study = optuna.create_study(direction="minimize", study_name="lgbm_hpo")
            study.optimize(_lgbm_objective, n_trials=n_trials, show_progress_bar=False)
            best_params["LightGBM"] = study.best_params
            logger.info("âœ“ LightGBM HPO tamamlandÄ± (logloss: %.4f)", study.best_value)
        except ImportError:
            pass
        except Exception as e:
            logger.warning("âš  LightGBM HPO hatasÄ±: %s", e)

        # â”€â”€ XGBoost HPO â”€â”€
        try:
            from xgboost import XGBClassifier

            def _xgb_objective(trial: optuna.Trial) -> float:
                params = {
                    "n_estimators": trial.suggest_int("n_estimators", 100, 500),
                    "max_depth": trial.suggest_int("max_depth", 3, 10),
                    "learning_rate": trial.suggest_float("learning_rate", 0.01, 0.3, log=True),
                    "subsample": trial.suggest_float("subsample", 0.5, 1.0),
                    "colsample_bytree": trial.suggest_float("colsample_bytree", 0.5, 1.0),
                    "reg_alpha": trial.suggest_float("reg_alpha", 1e-3, 10.0, log=True),
                    "reg_lambda": trial.suggest_float("reg_lambda", 1e-3, 10.0, log=True),
                    "min_child_weight": trial.suggest_int("min_child_weight", 1, 10),
                    "gamma": trial.suggest_float("gamma", 0.0, 5.0),
                    "eval_metric": "mlogloss",
                    "use_label_encoder": False,
                    "random_state": RANDOM_SEED,
                    "verbosity": 0,
                    "n_jobs": -1,
                }
                scores = []
                for train_idx, val_idx in tscv.split(X):
                    model = XGBClassifier(**params)
                    model.fit(X[train_idx], y[train_idx])
                    probs = model.predict_proba(X[val_idx])
                    scores.append(log_loss(y[val_idx], probs))
                return float(np.mean(scores))

            study = optuna.create_study(direction="minimize", study_name="xgb_hpo")
            study.optimize(_xgb_objective, n_trials=n_trials, show_progress_bar=False)
            best_params["XGBoost"] = study.best_params
            logger.info("âœ“ XGBoost HPO tamamlandÄ± (logloss: %.4f)", study.best_value)
        except ImportError:
            pass
        except Exception as e:
            logger.warning("âš  XGBoost HPO hatasÄ±: %s", e)

        return best_params

    def _init_base_models(
        self, cat_features: Optional[List[Dict[str, str]]] = None,
    ) -> None:
        """KullanÄ±labilir ML kÃ¼tÃ¼phanelerine gÃ¶re base model listesini oluÅŸturur.
        Optuna ile bulunan en iyi parametreler varsa onlarÄ± kullanÄ±r."""
        # â”€â”€ CatBoost â”€â”€
        try:
            from catboost import CatBoostClassifier
            if "CatBoost" in self._best_params:
                hp = self._best_params["CatBoost"]
                model = CatBoostClassifier(
                    iterations=hp.get("iterations", 300),
                    depth=hp.get("depth", 6),
                    learning_rate=hp.get("learning_rate", 0.08),
                    l2_leaf_reg=hp.get("l2_leaf_reg", 3.0),
                    bagging_temperature=hp.get("bagging_temperature", 0.5),
                    loss_function="MultiClass",
                    eval_metric="Accuracy",
                    random_seed=RANDOM_SEED,
                    verbose=0,
                    early_stopping_rounds=30,
                    use_best_model=True,
                )
                logger.info("âœ“ CatBoost yÃ¼klendi (Optuna optimized)")
            else:
                model = CatBoostClassifier(
                    iterations=300,
                    depth=6,
                    learning_rate=0.08,
                    loss_function="MultiClass",
                    eval_metric="Accuracy",
                    random_seed=RANDOM_SEED,
                    verbose=0,
                    early_stopping_rounds=30,
                    use_best_model=True,
                )
                logger.info("âœ“ CatBoost yÃ¼klendi (varsayÄ±lan parametreler)")
            self.base_models.append(("CatBoost", model))
            self._has_catboost = True
        except ImportError:
            logger.warning("âš  CatBoost bulunamadÄ±, atlanÄ±yor")

        # â”€â”€ LightGBM â”€â”€
        try:
            from lightgbm import LGBMClassifier
            if "LightGBM" in self._best_params:
                hp = self._best_params["LightGBM"]
                model = LGBMClassifier(
                    n_estimators=hp.get("n_estimators", 300),
                    max_depth=hp.get("max_depth", 6),
                    learning_rate=hp.get("learning_rate", 0.08),
                    num_leaves=hp.get("num_leaves", 31),
                    subsample=hp.get("subsample", 0.8),
                    colsample_bytree=hp.get("colsample_bytree", 0.8),
                    reg_alpha=hp.get("reg_alpha", 0.0),
                    reg_lambda=hp.get("reg_lambda", 0.0),
                    min_child_samples=hp.get("min_child_samples", 20),
                    objective="multiclass",
                    num_class=3,
                    random_state=RANDOM_SEED,
                    verbose=-1,
                    n_jobs=-1,
                )
                logger.info("âœ“ LightGBM yÃ¼klendi (Optuna optimized)")
            else:
                model = LGBMClassifier(
                    n_estimators=300,
                    max_depth=6,
                    learning_rate=0.08,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    num_leaves=31,
                    objective="multiclass",
                    num_class=3,
                    random_state=RANDOM_SEED,
                    verbose=-1,
                    n_jobs=-1,
                )
                logger.info("âœ“ LightGBM yÃ¼klendi (varsayÄ±lan parametreler)")
            self.base_models.append(("LightGBM", model))
            self._has_lightgbm = True
        except ImportError:
            logger.warning("âš  LightGBM bulunamadÄ±, atlanÄ±yor")

        # â”€â”€ XGBoost â”€â”€
        try:
            from xgboost import XGBClassifier
            if "XGBoost" in self._best_params:
                hp = self._best_params["XGBoost"]
                model = XGBClassifier(
                    n_estimators=hp.get("n_estimators", 300),
                    max_depth=hp.get("max_depth", 6),
                    learning_rate=hp.get("learning_rate", 0.08),
                    subsample=hp.get("subsample", 0.8),
                    colsample_bytree=hp.get("colsample_bytree", 0.8),
                    reg_alpha=hp.get("reg_alpha", 0.0),
                    reg_lambda=hp.get("reg_lambda", 0.0),
                    min_child_weight=hp.get("min_child_weight", 1),
                    gamma=hp.get("gamma", 0.0),
                    eval_metric="mlogloss",
                    use_label_encoder=False,
                    random_state=RANDOM_SEED,
                    verbosity=0,
                    n_jobs=-1,
                )
                logger.info("âœ“ XGBoost yÃ¼klendi (Optuna optimized)")
            else:
                model = XGBClassifier(
                    n_estimators=300,
                    max_depth=6,
                    learning_rate=0.08,
                    subsample=0.8,
                    colsample_bytree=0.8,
                    eval_metric="mlogloss",
                    use_label_encoder=False,
                    random_state=RANDOM_SEED,
                    verbosity=0,
                    n_jobs=-1,
                )
                logger.info("âœ“ XGBoost yÃ¼klendi (varsayÄ±lan parametreler)")
            self.base_models.append(("XGBoost", model))
            self._has_xgboost = True
        except ImportError:
            logger.warning("âš  XGBoost bulunamadÄ±, atlanÄ±yor")

        # â”€â”€ Fallback: En az bir model olmalÄ± â”€â”€
        if not self.base_models:
            from sklearn.ensemble import (
                RandomForestClassifier,
                GradientBoostingClassifier,
            )
            self.base_models.append((
                "RandomForest",
                RandomForestClassifier(
                    n_estimators=200, max_depth=10,
                    random_state=RANDOM_SEED, n_jobs=-1,
                ),
            ))
            self.base_models.append((
                "GradientBoosting",
                GradientBoostingClassifier(
                    n_estimators=200, max_depth=6,
                    learning_rate=0.1, random_state=RANDOM_SEED,
                ),
            ))
            logger.info("âš  Fallback modeller yÃ¼klendi: RandomForest + GradientBoosting")

    def _prepare_catboost_data(
        self,
        X_train: np.ndarray,
        X_val: np.ndarray,
        cat_features: List[Dict[str, str]],
        train_idx: np.ndarray,
        val_idx: np.ndarray,
    ) -> Tuple[np.ndarray, np.ndarray]:
        """CatBoost iÃ§in kategorik feature'larÄ± numeric X'e ekler.
        Åimdilik basit label encoding yapar (CatBoost zaten native handle eder).
        """
        # CatBoost native categorical kullanmak yerine sadece numeric kullanÄ±yoruz
        # Ã§Ã¼nkÃ¼ OOF stacking'de categorical handling karmaÅŸÄ±k.
        # CatBoost zaten tree-based olarak bunlarÄ± Ã¶ÄŸrenebilir.
        return X_train, X_val

    def _prepare_catboost_full(
        self,
        X: np.ndarray,
        cat_features: List[Dict[str, str]],
    ) -> np.ndarray:
        """TÃ¼m veri iÃ§in CatBoost hazÄ±rlÄ±ÄŸÄ±."""
        return X


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  SHAP Explainer Wrapper
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class SHAPExplainer:
    """SHAP ile tahmin aÃ§Ä±klamasÄ± Ã¼retir.

    Base modellerdeki en gÃ¼Ã§lÃ¼ aÄŸaÃ§ modelini (XGBoost > LightGBM > CatBoost)
    kullanarak SHAP deÄŸerleri hesaplar.

    KullanÄ±m::

        explainer = SHAPExplainer()
        explainer.initialize(stacking_ensemble.base_models, X_background)
        top_features = explainer.explain(feature_vector, prediction_class=0)
    """

    def __init__(self) -> None:
        self._explainer: Any = None
        self._feature_names: List[str] = FeatureExtractor.FEATURE_NAMES
        self._available: bool = False

    def initialize(
        self,
        base_models: List[Tuple[str, Any]],
        X_background: Optional[np.ndarray] = None,
    ) -> bool:
        """SHAP Explainer'Ä± baÅŸlatÄ±r.

        Parameters
        ----------
        base_models : Stacking'deki (name, model) Ã§iftleri
        X_background : SHAP background veri seti (100 satÄ±r yeterli)

        Returns
        -------
        bool
            SHAP baÅŸarÄ±yla initialize edildiyse True.
        """
        try:
            import shap

            # AÄŸaÃ§ modeli prioritize et
            tree_model = None
            for name, model in base_models:
                if name in ("XGBoost", "LightGBM", "CatBoost"):
                    tree_model = model
                    break

            if tree_model is None:
                logger.warning("âš  SHAP: AÄŸaÃ§ tabanlÄ± model bulunamadÄ±")
                return False

            if X_background is not None and len(X_background) > 100:
                bg = X_background[:100]
            else:
                bg = X_background

            self._explainer = shap.TreeExplainer(tree_model, data=bg)
            self._available = True
            logger.info("âœ“ SHAP TreeExplainer baÅŸlatÄ±ldÄ±")
            return True

        except ImportError:
            logger.warning("âš  shap kÃ¼tÃ¼phanesi bulunamadÄ±")
            return False
        except Exception as e:
            logger.warning("âš  SHAP baÅŸlatma hatasÄ±: %s", e)
            return False

    def explain(
        self,
        X: np.ndarray,
        prediction_class: int = 0,
        top_n: int = 5,
    ) -> Tuple[List[Tuple[str, float]], str]:
        """Tek bir tahmin iÃ§in SHAP deÄŸerlerini hesaplar.

        Parameters
        ----------
        X : (96,) feature vektÃ¶rÃ¼
        prediction_class : 0=MS1, 1=MSX, 2=MS2
        top_n : DÃ¶ndÃ¼rÃ¼lecek en Ã¶nemli feature sayÄ±sÄ±

        Returns
        -------
        Tuple[List[Tuple[str, float]], str]
            (top_features, human_readable_summary)
        """
        if not self._available or self._explainer is None:
            return [], ""

        try:
            import shap as _shap  # noqa: F811

            X_reshaped = X.reshape(1, -1) if X.ndim == 1 else X
            shap_values = self._explainer.shap_values(X_reshaped)

            # shap_values â†’ multiclass: list of arrays, her class iÃ§in (1, 85)
            if isinstance(shap_values, list):
                # Tahmin edilen class'Ä±n SHAP deÄŸerleri
                class_shap = shap_values[prediction_class][0]
            elif shap_values.ndim == 3:
                # (1, 85, 3) format
                class_shap = shap_values[0, :, prediction_class]
            else:
                class_shap = shap_values[0]

            # En etkili feature'larÄ± bul
            abs_shap = np.abs(class_shap)
            top_indices = np.argsort(abs_shap)[::-1][:top_n]

            top_features: List[Tuple[str, float]] = []
            for idx in top_indices:
                name = (
                    self._feature_names[idx]
                    if idx < len(self._feature_names)
                    else f"feature_{idx}"
                )
                top_features.append((name, float(class_shap[idx])))

            # Human-readable summary
            summary = self._build_shap_summary(
                top_features, prediction_class,
            )

            return top_features, summary

        except Exception as e:
            logger.warning("SHAP explain hatasÄ±: %s", e)
            return [], ""

    @staticmethod
    def _build_shap_summary(
        top_features: List[Tuple[str, float]],
        prediction_class: int,
    ) -> str:
        """SHAP deÄŸerlerinden insan okunabilir Ã¶zet Ã¼retir."""
        class_labels: Dict[int, str] = {0: "MS 1", 1: "MS X", 2: "MS 2"}
        label: str = class_labels.get(prediction_class, "?")

        # Feature isimleri â†’ TÃ¼rkÃ§e Ã§eviri
        name_map: Dict[str, str] = {
            "home_form_score": "Ev formu",
            "away_form_score": "Deplasman formu",
            "form_diff": "Form farkÄ±",
            "home_rank": "Ev sahibi sÄ±rasÄ±",
            "away_rank": "Deplasman sÄ±rasÄ±",
            "rank_diff": "SÄ±ra farkÄ±",
            "league_position_composite": "Lig pozisyonu composite",
            "ref_home_bias": "Hakem ev bias'Ä±",
            "ref_alignment_score": "Hakem uyumu",
            "h2h_home_win_rate": "H2H ev galibiyet oranÄ±",
            "h2h_recent_trend": "H2H son trend",
            "home_injury_penalty": "Ev eksik cezasÄ±",
            "away_injury_penalty": "Dep eksik cezasÄ±",
            "injury_penalty_diff": "Eksik farkÄ±",
            "implied_prob_home": "Oran olasÄ±lÄ±ÄŸÄ± (ev)",
            "implied_prob_away": "Oran olasÄ±lÄ±ÄŸÄ± (dep)",
            "home_strength_composite": "Ev gÃ¼Ã§ composite",
            "away_strength_composite": "Dep gÃ¼Ã§ composite",
            "strength_diff": "GÃ¼Ã§ farkÄ±",
            "home_exp_decay_form": "Ev exp-decay form",
            "away_exp_decay_form": "Dep exp-decay form",
            "exp_decay_form_diff": "Exp-decay form farkÄ±",
            "exp_decay_momentum": "Form momentumu",
            "home_rolling3_scored": "Ev son 3 maÃ§ gol ort.",
            "away_rolling3_scored": "Dep son 3 maÃ§ gol ort.",
            "home_form_x_away_defense_weakness": "Ev form Ã— Dep defans zaf.",
            "away_form_x_home_defense_weakness": "Dep form Ã— Ev defans zaf.",
            "home_attack_x_away_concede": "Ev atak Ã— Dep gol yeme",
        }

        parts: List[str] = [f"ğŸ” Neden {label}?"]
        for feat_name, shap_val in top_features:
            direction: str = "â†‘" if shap_val > 0 else "â†“"
            tr_name: str = name_map.get(feat_name, feat_name)
            parts.append(f"  {direction} {tr_name} ({shap_val:+.3f})")

        return "\n".join(parts)


# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
#  Ana Tahmin SÄ±nÄ±fÄ±
# â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
class MatchPredictor:
    """Hibrit tahmin motoru v3.1 â€” Stacking Ensemble + SHAP + Bayesian Smoothing.

    v3.1 Dinamik Feature GÃ¼veni (Dynamic Feature Trust):
      Erken sezonda (< ~7 maÃ§) model otomatik olarak:
      â€¢ implied_prob (oran bazlÄ±) feature'lara DAHA FAZLA gÃ¼venir
      â€¢ standing (sÄ±ra, puan) feature'lara DAHA AZ gÃ¼venir
      â€¢ dampened_rank feature'larÄ± Bayesian shrinkage ile medyana Ã§ekilmiÅŸtir
      â€¢ season_confidence feature'Ä± modele bu konteksti saÄŸlar
      â€¢ Poisson Î» hesaplamasÄ±nda Bayesian Average kullanÄ±lÄ±r:
        bayesian = (observed Ã— n + prior Ã— C) / (n + C)

    KullanÄ±m::

        predictor = MatchPredictor(session)
        predictor.initialize()          # Modeli eÄŸit veya yÃ¼kle
        result = predictor.predict(match)
    """

    MODEL_FILE = MODEL_DIR / "match_predictor.pkl"
    MODEL_VERSION: str = "v3.1"

    def __init__(self, session: Session) -> None:
        self.session: Session = session
        self.extractor: FeatureExtractor = FeatureExtractor(session)
        self.poisson: PoissonModel = PoissonModel()
        self.stacking: Optional[StackingEnsemble] = None
        self.shap_explainer: SHAPExplainer = SHAPExplainer()
        self.training_samples: int = 0
        self._mode: str = "poisson"  # "poisson" | "hybrid" | "ml"
        self._X_background: Optional[np.ndarray] = None  # SHAP background data

    # â”€â”€â”€ BaÅŸlatma / EÄŸitim â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def initialize(self) -> str:
        """Modeli baÅŸlatÄ±r. Veri miktarÄ±na gÃ¶re mod seÃ§er.

        Returns
        -------
        str
            Aktif mod aÃ§Ä±klamasÄ±.
        """
        finished: int = (
            self.session.query(Match)
            .filter(Match.is_finished == True)  # noqa: E712
            .count()
        )
        self.training_samples = finished

        if finished < MIN_TRAINING_SAMPLES:
            self._mode = "poisson"
            logger.info(
                "ğŸ“Š Cold-Start modu: Poisson (%d/%d maÃ§)",
                finished, MIN_TRAINING_SAMPLES,
            )
            return f"Poisson (Cold-Start: {finished} maÃ§)"

        # KayÄ±tlÄ± model var mÄ± kontrol et
        if self._load_model():
            if finished >= MIN_TRAINING_SAMPLES_XGBOOST:
                self._mode = "ml"
            else:
                self._mode = "hybrid"
            logger.info("ğŸ“Š Model yÃ¼klendi: %s mod", self._mode)
            return f"{self._mode} (kayÄ±tlÄ± model, {finished} maÃ§)"

        # Yeni model eÄŸit
        return self._train_model(finished)

    def _train_model(self, finished: int) -> str:
        """Stacking Ensemble modelini eÄŸitir.

        v3.1:
          â€¢ 96-feature vektÃ¶rÃ¼ (feature_engineering v3.1 â€” 11 Bayesian dampening feature)
          â€¢ Stacking: CatBoost + LightGBM + XGBoost â†’ LogisticRegression
          â€¢ Temporal K-Fold CV (data leakage korumasÄ±)
          â€¢ SHAP TreeExplainer baÅŸlatma
        """
        logger.info("ğŸ”§ Stacking Ensemble eÄŸitiliyor (v3.0)...")

        # Eski modeli temizle (feature boyutu veya versiyon deÄŸiÅŸmiÅŸ olabilir)
        if self.MODEL_FILE.exists():
            self.MODEL_FILE.unlink()
            logger.info("ğŸ—‘  Eski model cache silindi (v3.0 upgrade)")

        # â”€â”€ Veri Ã§ek (kategorik dahil) â”€â”€
        X, y, cat_features = build_training_dataset_with_categorical(self.session)

        if len(X) < MIN_TRAINING_SAMPLES:
            self._mode = "poisson"
            return f"Poisson (yetersiz eÄŸitim verisi: {len(X)})"

        # â”€â”€ Stacking Ensemble eÄŸit â”€â”€
        self.stacking = StackingEnsemble(
            use_optuna=True,
            optuna_n_trials=30,
        )
        try:
            result = self.stacking.fit(
                X, y,
                cat_features=cat_features,
                n_splits=min(5, max(2, len(X) // 50)),  # Adaptif split
            )
        except Exception as e:
            logger.error("Stacking eÄŸitim hatasÄ±: %s", e)
            self._mode = "poisson"
            return f"Poisson (stacking hatasÄ±: {e})"

        # â”€â”€ SHAP baÅŸlat â”€â”€
        self._X_background = X[:100] if len(X) > 100 else X
        self.shap_explainer.initialize(
            self.stacking.base_models,
            self._X_background,
        )

        # â”€â”€ Model kaydet â”€â”€
        self._save_model()

        if finished >= MIN_TRAINING_SAMPLES_XGBOOST:
            self._mode = "ml"
        else:
            self._mode = "hybrid"

        # â”€â”€ SonuÃ§ raporu â”€â”€
        acc = result.get("stacking_accuracy", 0)
        base_info = []
        for key, val in result.items():
            if key.endswith("_avg_accuracy"):
                name = key.replace("_avg_accuracy", "")
                base_info.append(f"{name}: {val:.1%}")

        return (
            f"{self._mode} (Stacking Ensemble, "
            f"doÄŸruluk: {acc:.2%}, "
            f"base: [{', '.join(base_info)}], "
            f"{finished} maÃ§)"
        )

    def retrain(self) -> str:
        """Modeli yeniden eÄŸitir."""
        finished: int = (
            self.session.query(Match)
            .filter(Match.is_finished == True)  # noqa: E712
            .count()
        )
        self.training_samples = finished
        return self._train_model(finished)

    # â”€â”€â”€ Model Kaydetme / YÃ¼kleme â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # GÃœVENLÄ°K: pickle dosyasÄ± RCE riski taÅŸÄ±r. AÅŸaÄŸÄ±daki Ã¶nlemler uygulanÄ±r:
    #   1. YalnÄ±zca MODEL_DIR dizininden yÃ¼kleme (path traversal engeli)
    #   2. Kaydetme sÄ±rasÄ±nda SHA-256 hash Ã¼retilir (.sha256 dosyasÄ±)
    #   3. YÃ¼kleme sÄ±rasÄ±nda hash doÄŸrulanÄ±r â†’ uyumsuzlukta reddet
    #   4. GÃ¼venilmeyen ortamda uyarÄ± loglanÄ±r

    MODEL_HASH_FILE = MODEL_DIR / "match_predictor.pkl.sha256"

    @staticmethod
    def _compute_file_hash(filepath: Path) -> str:
        """DosyanÄ±n SHA-256 hash'ini hesaplar."""
        sha256 = hashlib.sha256()
        with open(filepath, "rb") as f:
            for chunk in iter(lambda: f.read(8192), b""):
                sha256.update(chunk)
        return sha256.hexdigest()

    def _save_model(self) -> None:
        """Modeli diske kaydeder + SHA-256 hash dosyasÄ± oluÅŸturur."""
        if self.stacking is None or not self.stacking.is_fitted:
            return
        try:
            with open(self.MODEL_FILE, "wb") as fp:
                pickle.dump(
                    {
                        "stacking": self.stacking,
                        "version": self.MODEL_VERSION,
                        "samples": self.training_samples,
                        "n_features": len(FeatureExtractor.FEATURE_NAMES),
                        "X_background": self._X_background,
                        "best_params": getattr(self.stacking, '_best_params', {}),
                    },
                    fp,
                )
            # SHA-256 hash kaydet
            file_hash = self._compute_file_hash(self.MODEL_FILE)
            self.MODEL_HASH_FILE.write_text(file_hash)
            logger.info("âœ“ Stacking model kaydedildi: %s (hash: %sâ€¦)", self.MODEL_FILE, file_hash[:12])
        except Exception as e:
            logger.error("Model kaydetme hatasÄ±: %s", e)

    def _load_model(self) -> bool:
        """KaydedilmiÅŸ modeli yÃ¼kler.

        GÃ¼venlik kontrolleri:
          1. Dosya yalnÄ±zca MODEL_DIR altÄ±ndaysa kabul edilir
          2. .sha256 hash dosyasÄ± mevcutsa doÄŸrulama yapÄ±lÄ±r
          3. Hash uyumsuzluÄŸu â†’ yÃ¼kleme reddedilir
          4. v3.0: Versiyon + feature boyutu uyumsuzluÄŸu kontrolÃ¼
        """
        if not self.MODEL_FILE.exists():
            return False

        # GÃ¼venlik: Sadece beklenen dizinden yÃ¼kle (path traversal engeli)
        try:
            resolved = self.MODEL_FILE.resolve()
            allowed_dir = MODEL_DIR.resolve()
            if not str(resolved).startswith(str(allowed_dir)):
                logger.error("â›” Model dosyasÄ± gÃ¼venli dizin dÄ±ÅŸÄ±nda: %s", resolved)
                return False
        except Exception:
            return False

        # Hash doÄŸrulama
        if self.MODEL_HASH_FILE.exists():
            expected_hash = self.MODEL_HASH_FILE.read_text().strip()
            actual_hash = self._compute_file_hash(self.MODEL_FILE)
            if expected_hash != actual_hash:
                logger.error(
                    "â›” Model dosyasÄ± hash doÄŸrulamasÄ± BAÅARISIZ!\n"
                    "  Beklenen: %s\n  Bulunan:  %s\n"
                    "  Dosya deÄŸiÅŸtirilmiÅŸ olabilir. Model yeniden eÄŸitilecek.",
                    expected_hash, actual_hash,
                )
                self.MODEL_FILE.unlink(missing_ok=True)
                self.MODEL_HASH_FILE.unlink(missing_ok=True)
                return False
            logger.debug("âœ“ Model hash doÄŸrulandÄ±: %sâ€¦", actual_hash[:12])
        else:
            logger.warning(
                "âš ï¸  Model hash dosyasÄ± bulunamadÄ± (%s). "
                "Model yeniden eÄŸitilecek (gÃ¼venlik Ã¶nlemi).",
                self.MODEL_HASH_FILE,
            )
            self.MODEL_FILE.unlink(missing_ok=True)
            return False

        try:
            with open(self.MODEL_FILE, "rb") as fp:
                data: dict = pickle.load(fp)  # noqa: S301
            saved_version: str = data.get("version", "v1.0")
            saved_n_features: int = data.get("n_features", 0)
            expected_n: int = len(FeatureExtractor.FEATURE_NAMES)

            # Versiyon veya feature boyutu uyumsuzluÄŸu â†’ yeniden eÄŸit
            if saved_version != self.MODEL_VERSION or saved_n_features != expected_n:
                logger.info(
                    "âš ï¸  Model uyumsuz (v=%sâ†’%s, feat=%dâ†’%d), yeniden eÄŸitilecek",
                    saved_version, self.MODEL_VERSION,
                    saved_n_features, expected_n,
                )
                self.MODEL_FILE.unlink()
                self.MODEL_HASH_FILE.unlink(missing_ok=True)
                return False

            self.stacking = data["stacking"]
            self._X_background = data.get("X_background")
            logger.info("âœ“ Stacking model yÃ¼klendi (versiyon: %s)", saved_version)

            # SHAP'Ä± yeniden baÅŸlat
            if self.stacking and self.stacking.base_models:
                self.shap_explainer.initialize(
                    self.stacking.base_models,
                    self._X_background,
                )

            return True
        except Exception as e:
            logger.warning("Model yÃ¼kleme hatasÄ±: %s", e)
            return False

    # â”€â”€â”€ Tahmin â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def predict(self, match: Match) -> PredictionResult:
        """Bir maÃ§ iÃ§in tahmin Ã¼retir.

        Mod'a gÃ¶re Poisson, Stacking veya hibrit kullanÄ±r.
        SHAP aÃ§Ä±klamalarÄ± otomatik eklenir (mÃ¼mkÃ¼nse).
        """
        features: Dict[str, float] = self.extractor.extract(match)
        feature_vector: np.ndarray = self.extractor.extract_vector(match)

        # Poisson tahmini (her zaman hesapla)
        poisson_result: PoissonResult = self.poisson.predict_from_features(features)

        # Oran bilgisi
        odds: Optional[Odds] = (
            self.session.query(Odds).filter_by(match_id=match.id).first()
        )

        if self._mode == "poisson":
            return self._build_poisson_prediction(
                match, poisson_result, features, odds,
            )
        elif self._mode == "hybrid":
            return self._build_hybrid_prediction(
                match, poisson_result, feature_vector, features, odds,
            )
        else:  # ml
            return self._build_ml_prediction(
                match, poisson_result, feature_vector, features, odds,
            )

    def predict_batch(self, matches: List[Match]) -> List[PredictionResult]:
        """Birden fazla maÃ§ iÃ§in toplu tahmin."""
        results: List[PredictionResult] = []
        for match in matches:
            try:
                results.append(self.predict(match))
            except Exception as e:
                logger.error("Tahmin hatasÄ± (%s): %s", match.display_name, e)
        return results

    # â”€â”€â”€ Tahmin OluÅŸturucularÄ± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _build_poisson_prediction(
        self,
        match: Match,
        pr: PoissonResult,
        features: Dict[str, float],
        odds: Optional[Odds],
    ) -> PredictionResult:
        """Saf Poisson tahmini."""
        prediction: str = pr.prediction
        probs: Dict[str, float] = {
            "1": pr.prob_home, "X": pr.prob_draw, "2": pr.prob_away,
        }
        confidence: float = probs[prediction]

        edge, is_value = self._calc_value_edge(prediction, probs, odds)
        risk: str = self._determine_risk(confidence, features, prediction)
        explanation: str = self._generate_explanation(
            features, pr, prediction, confidence, "poisson",
        )

        return PredictionResult(
            match_id=match.id,
            match_display=match.display_name,
            engine_used="poisson",
            model_version=f"poisson_{self.MODEL_VERSION}",
            prob_home=pr.prob_home,
            prob_draw=pr.prob_draw,
            prob_away=pr.prob_away,
            prob_over_25=pr.prob_over_25,
            prob_under_25=pr.prob_under_25,
            expected_home_goals=pr.expected_home_goals,
            expected_away_goals=pr.expected_away_goals,
            top_scores=pr.top_scores,
            prediction=prediction,
            confidence=confidence,
            value_edge=edge,
            is_value_bet=is_value,
            risk_level=risk,
            explanation=explanation,
        )

    def _build_ml_prediction(
        self,
        match: Match,
        pr: PoissonResult,
        feature_vec: np.ndarray,
        features: Dict[str, float],
        odds: Optional[Odds],
    ) -> PredictionResult:
        """Stacking Ensemble aÄŸÄ±rlÄ±klÄ± tahmin (Poisson doÄŸrulamasÄ± ile)."""
        if self.stacking is None or not self.stacking.is_fitted:
            return self._build_poisson_prediction(match, pr, features, odds)

        vec: np.ndarray = np.nan_to_num(feature_vec.reshape(1, -1), nan=0.0)
        ml_probs: np.ndarray = self.stacking.predict_proba(vec)[0]

        labels: List[str] = ["1", "X", "2"]
        ml_prob_dict: Dict[str, float] = {
            labels[i]: ml_probs[i] * 100 for i in range(len(labels))
        }

        # Stacking (%70) + Poisson (%30)
        probs: Dict[str, float] = {
            "1": ml_prob_dict["1"] * 0.70 + pr.prob_home * 0.30,
            "X": ml_prob_dict["X"] * 0.70 + pr.prob_draw * 0.30,
            "2": ml_prob_dict["2"] * 0.70 + pr.prob_away * 0.30,
        }

        prediction: str = max(probs, key=probs.get)  # type: ignore[arg-type]
        confidence: float = probs[prediction]

        edge, is_value = self._calc_value_edge(prediction, probs, odds)
        risk: str = self._determine_risk(confidence, features, prediction)

        # â”€â”€ SHAP aÃ§Ä±klama â”€â”€
        label_to_class: Dict[str, int] = {"1": 0, "X": 1, "2": 2}
        shap_features, shap_summary = self.shap_explainer.explain(
            feature_vec,
            prediction_class=label_to_class.get(prediction, 0),
        )

        explanation: str = self._generate_explanation(
            features, pr, prediction, confidence, "ml",
            shap_summary=shap_summary,
        )

        return PredictionResult(
            match_id=match.id,
            match_display=match.display_name,
            engine_used="ml",
            model_version=f"stacking_{self.MODEL_VERSION}",
            prob_home=probs["1"],
            prob_draw=probs["X"],
            prob_away=probs["2"],
            prob_over_25=pr.prob_over_25,
            prob_under_25=pr.prob_under_25,
            expected_home_goals=pr.expected_home_goals,
            expected_away_goals=pr.expected_away_goals,
            top_scores=pr.top_scores,
            prediction=prediction,
            confidence=confidence,
            value_edge=edge,
            is_value_bet=is_value,
            risk_level=risk,
            explanation=explanation,
            shap_top_features=shap_features,
            shap_summary=shap_summary,
        )

    def _build_hybrid_prediction(
        self,
        match: Match,
        pr: PoissonResult,
        feature_vec: np.ndarray,
        features: Dict[str, float],
        odds: Optional[Odds],
    ) -> PredictionResult:
        """Hibrit tahmin: Poisson (%60) + Stacking (%40)."""
        if self.stacking is None or not self.stacking.is_fitted:
            return self._build_poisson_prediction(match, pr, features, odds)

        vec: np.ndarray = np.nan_to_num(feature_vec.reshape(1, -1), nan=0.0)
        ml_probs: np.ndarray = self.stacking.predict_proba(vec)[0]

        labels: List[str] = ["1", "X", "2"]
        ml_prob_dict: Dict[str, float] = {
            labels[i]: ml_probs[i] * 100 for i in range(len(labels))
        }

        probs: Dict[str, float] = {
            "1": pr.prob_home * 0.60 + ml_prob_dict["1"] * 0.40,
            "X": pr.prob_draw * 0.60 + ml_prob_dict["X"] * 0.40,
            "2": pr.prob_away * 0.60 + ml_prob_dict["2"] * 0.40,
        }

        prediction: str = max(probs, key=probs.get)  # type: ignore[arg-type]
        confidence: float = probs[prediction]

        edge, is_value = self._calc_value_edge(prediction, probs, odds)
        risk: str = self._determine_risk(confidence, features, prediction)

        # â”€â”€ SHAP â”€â”€
        label_to_class: Dict[str, int] = {"1": 0, "X": 1, "2": 2}
        shap_features, shap_summary = self.shap_explainer.explain(
            feature_vec,
            prediction_class=label_to_class.get(prediction, 0),
        )

        explanation: str = self._generate_explanation(
            features, pr, prediction, confidence, "hybrid",
            shap_summary=shap_summary,
        )

        return PredictionResult(
            match_id=match.id,
            match_display=match.display_name,
            engine_used="hybrid",
            model_version=f"hybrid_stacking_{self.MODEL_VERSION}",
            prob_home=probs["1"],
            prob_draw=probs["X"],
            prob_away=probs["2"],
            prob_over_25=pr.prob_over_25,
            prob_under_25=pr.prob_under_25,
            expected_home_goals=pr.expected_home_goals,
            expected_away_goals=pr.expected_away_goals,
            top_scores=pr.top_scores,
            prediction=prediction,
            confidence=confidence,
            value_edge=edge,
            is_value_bet=is_value,
            risk_level=risk,
            explanation=explanation,
            shap_top_features=shap_features,
            shap_summary=shap_summary,
        )

    # â”€â”€â”€ YardÄ±mcÄ± Fonksiyonlar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def _calc_value_edge(
        self,
        prediction: str,
        probs: Dict[str, float],
        odds: Optional[Odds],
    ) -> Tuple[float, bool]:
        """Value edge hesaplar.

        ``edge = model_prob âˆ’ implied_prob``.
        ``edge â‰¥ VALUE_BET_MIN_EDGE`` â†’ value bet.
        """
        if not odds:
            return 0.0, False

        odd_map: Dict[str, Optional[float]] = {
            "1": odds.ms_1, "X": odds.ms_x, "2": odds.ms_2,
        }
        odd: Optional[float] = odd_map.get(prediction)

        if not odd or odd <= 1.0:
            return 0.0, False

        implied: float = (1.0 / odd) * 100
        model_prob: float = probs.get(prediction, 0)
        edge: float = model_prob - implied

        return round(edge, 2), edge >= VALUE_BET_MIN_EDGE

    def _determine_risk(
        self,
        confidence: float,
        features: Dict[str, float],
        prediction: str,
    ) -> str:
        """Risk seviyesi belirler.

        Risk faktÃ¶rleri (v3.1):
          1. GÃ¼ven seviyesi
          2. H2H veri eksikliÄŸi
          3. Toplam sakatlÄ±k etkisi
          4. Kritik eksik oyuncu
          5. Hakem bias
          6. Hakem-tahmin Ã§eliÅŸkisi
          7. Form trend tutarsÄ±zlÄ±ÄŸÄ±
          8. SakatlÄ±k normalize skoru
          9. (YENÄ°) Erken sezon gÃ¼venilirlik eksikliÄŸi

        Erken sezonda (< 7 hafta) puan tablosu gÃ¼venilmezdir.
        Bayesian Smoothing bunu azaltÄ±r ama ek risk olarak iÅŸaretlenir.

        Seviyeler:
          â‰¥ 6 â†’ ğŸ”´ YÃ¼ksek Risk
          â‰¥ 3 â†’ ğŸŸ¡ Orta Risk
          < 3 â†’ ğŸŸ¢ DÃ¼ÅŸÃ¼k Risk
        """
        risk_score: int = 0

        # 1 â€” GÃ¼ven seviyesi
        if confidence < 40:
            risk_score += 3
        elif confidence < 50:
            risk_score += 2
        elif confidence < 55:
            risk_score += 1

        # 2 â€” H2H verisi eksikliÄŸi
        h2h_total: float = features.get("h2h_total", 0)
        if h2h_total < 3:
            risk_score += 1

        # 3 â€” Toplam sakatlÄ±k etkisi
        total_injury: float = features.get("total_injury_importance", 0)
        if total_injury > 20:
            risk_score += 1

        # 4 â€” Kritik eksik oyuncu sayÄ±sÄ±
        critical_total: float = (
            features.get("home_critical_injury_count", 0)
            + features.get("away_critical_injury_count", 0)
        )
        if critical_total >= 3:
            risk_score += 2
        elif critical_total >= 1:
            risk_score += 1

        # 5 â€” GÃ¼Ã§lÃ¼ hakem bias'Ä±
        ref_bias: float = features.get("ref_home_bias", 0)
        if abs(ref_bias) > 15:
            risk_score += 1

        # 6 â€” Hakem-tahmin Ã§eliÅŸkisi
        ref_alignment: float = features.get("ref_alignment_score", 50)
        if ref_alignment < 25:
            risk_score += 1

        # 7 â€” Form trend tutarsÄ±zlÄ±ÄŸÄ±
        form_trend_diff: float = features.get("form_trend_diff", 0)
        if prediction == "1" and form_trend_diff < -0.5:
            risk_score += 1
        elif prediction == "2" and form_trend_diff > 0.5:
            risk_score += 1

        # 8 â€” SakatlÄ±k normalize skoru
        inj_norm: float = features.get("injury_normalized_score", 50)
        if inj_norm < 25 and prediction == "1":
            risk_score += 1
        elif inj_norm > 75 and prediction == "2":
            risk_score += 1

        # 9 â€” v3.1: Erken sezon gÃ¼venilirlik eksikliÄŸi
        # Puan tablosu verisi yetersiz â†’ tahmine gÃ¼venmek riskli.
        # Model Bayesian smoothing kullanÄ±yor ama yine de ek risk.
        early_reliability: float = features.get("early_season_reliability", 100.0)
        if early_reliability < 30:  # < ~3 maÃ§
            risk_score += 2
        elif early_reliability < 50:  # < ~5 maÃ§
            risk_score += 1

        # â”€â”€ Seviye belirleme â”€â”€
        if risk_score >= 6:
            return "ğŸ”´ YÃ¼ksek Risk"
        elif risk_score >= 3:
            return "ğŸŸ¡ Orta Risk"
        return "ğŸŸ¢ DÃ¼ÅŸÃ¼k Risk"

    def _generate_explanation(
        self,
        features: Dict[str, float],
        pr: PoissonResult,
        prediction: str,
        confidence: float,
        engine: str,
        shap_summary: str = "",
    ) -> str:
        """Ä°nsan okunabilir aÃ§Ä±klama Ã¼retir.

        v3.1: Erken sezon uyarÄ±sÄ± + piyasa gÃ¼cÃ¼ bilgisi eklendi.
        v3.0: SHAP Ã¶zeti aÃ§Ä±klamanÄ±n sonuna eklenir.
        """
        parts: List[str] = []

        # â”€â”€ Motor bilgisi â”€â”€
        engine_labels: Dict[str, str] = {
            "poisson": "ğŸ“Š Poisson",
            "hybrid": "ğŸ”€ Hibrit (Poisson+Stacking)",
            "ml": "ğŸ¤– Stacking Ensemble",
        }
        parts.append(f"[{engine_labels.get(engine, engine)}]")

        # â”€â”€ v3.1: Erken sezon uyarÄ±sÄ± â”€â”€
        early_reliability: float = features.get("early_season_reliability", 100.0)
        season_progress: float = features.get("season_progress", 1.0)
        if early_reliability < 50:
            hafta: int = max(int(season_progress * 34), 1)
            parts.append(
                f"âš ï¸ Erken sezon (Hafta ~{hafta}): "
                f"Puan tablosu gÃ¼venilirliÄŸi %{early_reliability:.0f} â€” "
                f"Bayesian sÃ¶nÃ¼mleme aktif, oranlar aÄŸÄ±rlÄ±klÄ±"
            )

        # â”€â”€ Poisson beklentileri â”€â”€
        parts.append(
            f"âš½ Beklenen: {pr.expected_home_goals:.1f}-{pr.expected_away_goals:.1f}"
        )

        # â”€â”€ v3.1: Piyasa gÃ¼cÃ¼ (erken sezonda Ã¶nemli) â”€â”€
        mkt_diff: float = features.get("market_strength_diff", 0)
        if abs(mkt_diff) > 10:
            mkt_favori: str = "Ev sahibi" if mkt_diff > 0 else "Deplasman"
            parts.append(
                f"ğŸ’° Piyasa gÃ¼cÃ¼: {mkt_favori} favori (fark: {abs(mkt_diff):.0f})"
            )

        # â”€â”€ Form + trend â”€â”€
        fd: float = features.get("form_diff", 0)
        trend_d: float = features.get("form_trend_diff", 0)
        if fd > 20:
            trend_icon: str = "â†‘â†‘" if trend_d > 0.3 else ("â†—" if trend_d > 0 else "â†’")
            parts.append(f"ğŸ“ˆ Ev sahibi formda {trend_icon}")
        elif fd < -20:
            trend_icon = "â†‘â†‘" if trend_d < -0.3 else ("â†—" if trend_d < 0 else "â†’")
            parts.append(f"ğŸ“ˆ Deplasman formda {trend_icon}")

        # â”€â”€ v3.0: Exponential Decay Momentum â”€â”€
        momentum: float = features.get("exp_decay_momentum", 0)
        if abs(momentum) > 10:
            m_team: str = "Ev sahibi" if momentum > 0 else "Deplasman"
            parts.append(f"ğŸš€ {m_team} yÃ¼kselen formda (+{abs(momentum):.0f})")

        # â”€â”€ Lig pozisyonu (Bayesian damped sÄ±ra kullan) â”€â”€
        # v3.1: Erken sezonda dampened_rank daha gÃ¼venilir,
        # geÃ§ sezonda home_rank'e yakÄ±n olacak.
        h_rank: float = features.get("dampened_home_rank",
                                     features.get("home_rank", 10))
        a_rank: float = features.get("dampened_away_rank",
                                     features.get("away_rank", 10))
        if abs(h_rank - a_rank) > 3:
            lider = "Ev sahibi" if h_rank < a_rank else "Deplasman"
            parts.append(
                f"ğŸ“ˆ {lider} ligde Ã¼stÃ¼n "
                f"({int(min(h_rank, a_rank))}. vs {int(max(h_rank, a_rank))}.)"
            )

        # â”€â”€ GÃ¼Ã§ composite'i â”€â”€
        strength_diff: float = features.get("strength_diff", 0)
        if abs(strength_diff) > 15:
            favori: str = "Ev sahibi" if strength_diff > 0 else "Deplasman"
            parts.append(f"ğŸ’ª {favori} gÃ¼Ã§lÃ¼ (+{abs(strength_diff):.0f})")

        # â”€â”€ Hakem bias + alignment â”€â”€
        ref_bias: float = features.get("ref_home_bias", 0)
        ref_align: float = features.get("ref_alignment_score", 50)
        if abs(ref_bias) > 10:
            bias_label: str = "evci" if ref_bias > 0 else "deplasmanÄ± destekler"
            align_note: str = ""
            if ref_align > 45:
                align_note = " âœ“ tahminle uyumlu"
            elif ref_align < 30:
                align_note = " âš  tahminle Ã§eliÅŸiyor"
            parts.append(f"ğŸ‘¨â€âš–ï¸ Hakem {bias_label} ({ref_bias:+.1f}){align_note}")

        # â”€â”€ Eksik oyuncular â”€â”€
        h_inj: float = features.get("home_injury_penalty", 0)
        a_inj: float = features.get("away_injury_penalty", 0)
        h_crit: int = int(features.get("home_critical_injury_count", 0))
        a_crit: int = int(features.get("away_critical_injury_count", 0))
        inj_norm: float = features.get("injury_normalized_score", 50)

        if h_inj > 5 or h_crit > 0:
            crit_txt: str = f" ({h_crit} kritik)" if h_crit else ""
            parts.append(f"ğŸ¥ Ev {h_inj:.0f}p eksik{crit_txt}")
        if a_inj > 5 or a_crit > 0:
            crit_txt = f" ({a_crit} kritik)" if a_crit else ""
            parts.append(f"ğŸ¥ Dep {a_inj:.0f}p eksik{crit_txt}")
        if abs(inj_norm - 50) > 15:
            avantaj: str = "ev sahibi" if inj_norm > 50 else "deplasman"
            parts.append(f"ğŸ¥ SaÄŸlÄ±k avantajÄ±: {avantaj}")

        # â”€â”€ H2H â”€â”€
        h2h_total: float = features.get("h2h_total", 0)
        if h2h_total >= 3:
            h2h_rate: float = features.get("h2h_home_win_rate", 33)
            h2h_trend: float = features.get("h2h_recent_trend", 0)
            h2h_uyum: float = features.get("h2h_tahmin_uyumu", 33.3)
            trend_str: str = ""
            if h2h_trend > 0.3:
                trend_str = " â†‘ ev trendi"
            elif h2h_trend < -0.3:
                trend_str = " â†‘ dep trendi"
            uyum_str: str = ""
            if h2h_uyum > 50:
                uyum_str = " âœ“ tahminle uyumlu"
            parts.append(
                f"ğŸ“œ H2H: {int(h2h_total)} maÃ§, ev %{h2h_rate:.0f}"
                f"{trend_str}{uyum_str}"
            )

        # â”€â”€ v3.0: Rolling Averages â”€â”€
        h_r3: float = features.get("home_rolling3_scored", 0)
        a_r3: float = features.get("away_rolling3_scored", 0)
        if h_r3 > 0 or a_r3 > 0:
            parts.append(
                f"ğŸ“Š Son 3 maÃ§ gol ort: Ev {h_r3:.1f} - Dep {a_r3:.1f}"
            )

        # â”€â”€ En olasÄ± skor â”€â”€
        if pr.top_scores:
            top = pr.top_scores[0]
            parts.append(f"ğŸ¯ En olasÄ± skor: {top[0]} (%{top[1]:.1f})")

        # â”€â”€ v3.0: SHAP Ã¶zeti â”€â”€
        if shap_summary:
            parts.append(f"\n{shap_summary}")

        return " | ".join(parts)

    # â”€â”€â”€ Model DoÄŸrulama â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def validate_past_predictions(self) -> Dict[str, object]:
        """GeÃ§miÅŸ tahminlerin doÄŸruluÄŸunu hesaplar."""
        preds: List[Prediction] = (
            self.session.query(Prediction)
            .filter(Prediction.actual_result.isnot(None))
            .all()
        )

        if not preds:
            return {"total": 0, "correct": 0, "accuracy": 0.0}

        correct: int = sum(1 for p in preds if p.is_correct)
        total: int = len(preds)

        by_engine: Dict[str, Dict[str, int | float]] = {}
        for p in preds:
            eng: str = p.engine_used or "unknown"
            if eng not in by_engine:
                by_engine[eng] = {"total": 0, "correct": 0}
            by_engine[eng]["total"] += 1  # type: ignore[operator]
            if p.is_correct:
                by_engine[eng]["correct"] += 1  # type: ignore[operator]

        for eng in by_engine:
            t: int = by_engine[eng]["total"]  # type: ignore[assignment]
            c: int = by_engine[eng]["correct"]  # type: ignore[assignment]
            by_engine[eng]["accuracy"] = (c / t * 100) if t > 0 else 0.0

        return {
            "total": total,
            "correct": correct,
            "accuracy": (correct / total * 100) if total > 0 else 0.0,
            "by_engine": by_engine,
        }
